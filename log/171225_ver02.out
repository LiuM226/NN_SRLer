2017-12-25 21:06:34,283 : Finished building computation graph for training.
2017-12-25 21:06:36,698 : Read in training data successfully.
2017-12-25 21:06:36.718321: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2017-12-25 21:06:37.938761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.873
pciBusID: 0000:03:00.0
totalMemory: 7.92GiB freeMemory: 7.80GiB
2017-12-25 21:06:37.938794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)
2017-12-25 21:06:38,558 : Epoch 0 started:
2017-12-25 21:06:39,038 : Iter 0, training loss: 426.739922
2017-12-25 21:06:40,870 : Iter 10, training loss: 201.435524
2017-12-25 21:06:42,797 : Iter 20, training loss: 128.398399
2017-12-25 21:06:44,799 : Iter 30, training loss: 99.412107
2017-12-25 21:06:46,686 : Iter 40, training loss: 84.853723
2017-12-25 21:06:48,724 : Iter 50, training loss: 74.525480
2017-12-25 21:06:50,640 : Iter 60, training loss: 67.516673
2017-12-25 21:06:52,585 : Iter 70, training loss: 62.957634
2017-12-25 21:06:54,414 : Iter 80, training loss: 58.822731
2017-12-25 21:06:56,175 : Iter 90, training loss: 56.216101
2017-12-25 21:06:58,254 : Iter 100, training loss: 53.487941
2017-12-25 21:07:00,181 : Iter 110, training loss: 51.332855
2017-12-25 21:07:01,952 : Iter 120, training loss: 49.452527
2017-12-25 21:07:03,674 : Iter 130, training loss: 48.011361
2017-12-25 21:07:05,609 : Iter 140, training loss: 46.574826
2017-12-25 21:07:07,488 : Iter 150, training loss: 45.278613
2017-12-25 21:07:09,315 : Iter 160, training loss: 44.010271
2017-12-25 21:07:11,247 : Iter 170, training loss: 43.008581
2017-12-25 21:07:13,183 : Iter 180, training loss: 42.154006
2017-12-25 21:07:15,057 : Iter 190, training loss: 41.383353
2017-12-25 21:07:16,967 : Iter 200, training loss: 40.694873
2017-12-25 21:07:18,873 : Iter 210, training loss: 39.952923
2017-12-25 21:07:20,874 : Iter 220, training loss: 39.203164
2017-12-25 21:07:22,763 : Iter 230, training loss: 38.572440
2017-12-25 21:07:24,432 : Iter 240, training loss: 38.052897
2017-12-25 21:07:26,447 : Iter 250, training loss: 37.498823
2017-12-25 21:07:28,316 : Iter 260, training loss: 37.033025
2017-12-25 21:07:30,008 : Iter 270, training loss: 36.640570
2017-12-25 21:07:31,817 : Iter 280, training loss: 36.182074
2017-12-25 21:07:33,745 : Iter 290, training loss: 35.744372
2017-12-25 21:07:35,752 : Iter 300, training loss: 35.363649
2017-12-25 21:07:37,551 : Iter 310, training loss: 35.002483
2017-12-25 21:07:39,409 : Iter 320, training loss: 34.660508
2017-12-25 21:07:41,273 : Iter 330, training loss: 34.307536
2017-12-25 21:07:43,218 : Iter 340, training loss: 33.994186
2017-12-25 21:07:44,980 : Iter 350, training loss: 33.607431
2017-12-25 21:07:46,053 : Epoch 0, training loss: 33.437628
2017-12-25 21:07:47,218 : Epoch 0, validation loss: 23.140686
2017-12-25 21:07:47,345 : Training checkpoint has been saved.
2017-12-25 21:07:47,345 : Epoch 1 started:
2017-12-25 21:07:47,555 : Iter 0, training loss: 20.042335
2017-12-25 21:07:49,379 : Iter 10, training loss: 20.953331
2017-12-25 21:07:51,239 : Iter 20, training loss: 21.098486
2017-12-25 21:07:53,379 : Iter 30, training loss: 21.389906
2017-12-25 21:07:55,379 : Iter 40, training loss: 21.001308
2017-12-25 21:07:57,184 : Iter 50, training loss: 20.787474
2017-12-25 21:07:59,178 : Iter 60, training loss: 20.697003
2017-12-25 21:08:01,264 : Iter 70, training loss: 20.852834
2017-12-25 21:08:03,094 : Iter 80, training loss: 20.829175
2017-12-25 21:08:04,907 : Iter 90, training loss: 20.850299
2017-12-25 21:08:06,769 : Iter 100, training loss: 20.671171
2017-12-25 21:08:08,719 : Iter 110, training loss: 20.824434
2017-12-25 21:08:10,576 : Iter 120, training loss: 21.055712
2017-12-25 21:08:12,426 : Iter 130, training loss: 20.969843
2017-12-25 21:08:14,308 : Iter 140, training loss: 20.927125
2017-12-25 21:08:16,149 : Iter 150, training loss: 20.869636
2017-12-25 21:08:17,906 : Iter 160, training loss: 20.825822
2017-12-25 21:08:19,831 : Iter 170, training loss: 20.898515
2017-12-25 21:08:21,634 : Iter 180, training loss: 20.772463
2017-12-25 21:08:23,498 : Iter 190, training loss: 20.722197
2017-12-25 21:08:25,317 : Iter 200, training loss: 20.706921
2017-12-25 21:08:27,168 : Iter 210, training loss: 20.583871
2017-12-25 21:08:28,947 : Iter 220, training loss: 20.714696
2017-12-25 21:08:30,873 : Iter 230, training loss: 20.620103
2017-12-25 21:08:32,810 : Iter 240, training loss: 20.605816
2017-12-25 21:08:34,570 : Iter 250, training loss: 20.515977
2017-12-25 21:08:36,531 : Iter 260, training loss: 20.485669
2017-12-25 21:08:38,309 : Iter 270, training loss: 20.405500
2017-12-25 21:08:40,072 : Iter 280, training loss: 20.323690
2017-12-25 21:08:42,058 : Iter 290, training loss: 20.213016
2017-12-25 21:08:43,908 : Iter 300, training loss: 20.168924
2017-12-25 21:08:45,780 : Iter 310, training loss: 20.101807
2017-12-25 21:08:47,557 : Iter 320, training loss: 20.036454
2017-12-25 21:08:49,461 : Iter 330, training loss: 19.980323
2017-12-25 21:08:51,241 : Iter 340, training loss: 19.956676
2017-12-25 21:08:53,275 : Iter 350, training loss: 19.916626
2017-12-25 21:08:54,264 : Epoch 1, training loss: 19.916487
2017-12-25 21:08:55,404 : Epoch 1, validation loss: 20.539755
2017-12-25 21:08:55,469 : Training checkpoint has been saved.
2017-12-25 21:08:55,469 : Epoch 2 started:
2017-12-25 21:08:55,669 : Iter 0, training loss: 16.747875
2017-12-25 21:08:57,484 : Iter 10, training loss: 17.078491
2017-12-25 21:08:59,468 : Iter 20, training loss: 16.917084
2017-12-25 21:09:01,490 : Iter 30, training loss: 16.518856
2017-12-25 21:09:03,408 : Iter 40, training loss: 16.563693
2017-12-25 21:09:05,415 : Iter 50, training loss: 16.774737
2017-12-25 21:09:07,552 : Iter 60, training loss: 16.410490
2017-12-25 21:09:09,485 : Iter 70, training loss: 16.550578
2017-12-25 21:09:11,275 : Iter 80, training loss: 16.644905
2017-12-25 21:09:13,203 : Iter 90, training loss: 16.594249
2017-12-25 21:09:15,053 : Iter 100, training loss: 16.414708
2017-12-25 21:09:16,903 : Iter 110, training loss: 16.581309
2017-12-25 21:09:18,671 : Iter 120, training loss: 16.811624
2017-12-25 21:09:20,411 : Iter 130, training loss: 16.901367
2017-12-25 21:09:22,407 : Iter 140, training loss: 16.860076
2017-12-25 21:09:24,438 : Iter 150, training loss: 16.877754
2017-12-25 21:09:26,407 : Iter 160, training loss: 16.931828
2017-12-25 21:09:28,351 : Iter 170, training loss: 16.895392
2017-12-25 21:09:30,323 : Iter 180, training loss: 16.947135
2017-12-25 21:09:32,295 : Iter 190, training loss: 16.881352
2017-12-25 21:09:34,282 : Iter 200, training loss: 16.885536
2017-12-25 21:09:36,214 : Iter 210, training loss: 16.883421
2017-12-25 21:09:38,093 : Iter 220, training loss: 16.954686
2017-12-25 21:09:39,936 : Iter 230, training loss: 17.084540
2017-12-25 21:09:41,699 : Iter 240, training loss: 17.101731
2017-12-25 21:09:43,531 : Iter 250, training loss: 17.126069
2017-12-25 21:09:45,412 : Iter 260, training loss: 17.160878
2017-12-25 21:09:47,374 : Iter 270, training loss: 17.139996
2017-12-25 21:09:49,231 : Iter 280, training loss: 17.076512
2017-12-25 21:09:50,938 : Iter 290, training loss: 17.040426
2017-12-25 21:09:52,806 : Iter 300, training loss: 17.000546
2017-12-25 21:09:54,651 : Iter 310, training loss: 16.976484
2017-12-25 21:09:56,498 : Iter 320, training loss: 16.969200
2017-12-25 21:09:58,323 : Iter 330, training loss: 16.931723
2017-12-25 21:10:00,248 : Iter 340, training loss: 16.908426
2017-12-25 21:10:02,080 : Iter 350, training loss: 16.877826
2017-12-25 21:10:03,015 : Epoch 2, training loss: 16.856896
2017-12-25 21:10:04,156 : Epoch 2, validation loss: 18.297261
2017-12-25 21:10:04,228 : Training checkpoint has been saved.
2017-12-25 21:10:04,228 : Epoch 3 started:
2017-12-25 21:10:04,413 : Iter 0, training loss: 14.067114
2017-12-25 21:10:06,334 : Iter 10, training loss: 14.111874
2017-12-25 21:10:08,155 : Iter 20, training loss: 14.288664
2017-12-25 21:10:10,011 : Iter 30, training loss: 14.313055
2017-12-25 21:10:12,000 : Iter 40, training loss: 14.403288
2017-12-25 21:10:13,702 : Iter 50, training loss: 14.270428
2017-12-25 21:10:15,484 : Iter 60, training loss: 14.317539
2017-12-25 21:10:17,366 : Iter 70, training loss: 14.316482
2017-12-25 21:10:19,094 : Iter 80, training loss: 14.487722
2017-12-25 21:10:20,905 : Iter 90, training loss: 14.561482
2017-12-25 21:10:22,916 : Iter 100, training loss: 14.571435
2017-12-25 21:10:24,675 : Iter 110, training loss: 14.548655
2017-12-25 21:10:26,527 : Iter 120, training loss: 14.479592
2017-12-25 21:10:28,182 : Iter 130, training loss: 14.426019
2017-12-25 21:10:29,992 : Iter 140, training loss: 14.322198
2017-12-25 21:10:31,819 : Iter 150, training loss: 14.295177
2017-12-25 21:10:33,864 : Iter 160, training loss: 14.331608
2017-12-25 21:10:35,892 : Iter 170, training loss: 14.313431
2017-12-25 21:10:37,740 : Iter 180, training loss: 14.317013
2017-12-25 21:10:39,528 : Iter 190, training loss: 14.381619
2017-12-25 21:10:41,582 : Iter 200, training loss: 14.402588
2017-12-25 21:10:43,590 : Iter 210, training loss: 14.435552
2017-12-25 21:10:45,455 : Iter 220, training loss: 14.466421
2017-12-25 21:10:47,226 : Iter 230, training loss: 14.469676
2017-12-25 21:10:49,168 : Iter 240, training loss: 14.480516
2017-12-25 21:10:51,108 : Iter 250, training loss: 14.537333
2017-12-25 21:10:53,081 : Iter 260, training loss: 14.539340
2017-12-25 21:10:55,040 : Iter 270, training loss: 14.559688
2017-12-25 21:10:56,944 : Iter 280, training loss: 14.599536
2017-12-25 21:10:58,836 : Iter 290, training loss: 14.552517
2017-12-25 21:11:00,653 : Iter 300, training loss: 14.570543
2017-12-25 21:11:02,600 : Iter 310, training loss: 14.606286
2017-12-25 21:11:04,430 : Iter 320, training loss: 14.668775
2017-12-25 21:11:06,367 : Iter 330, training loss: 14.689266
2017-12-25 21:11:08,239 : Iter 340, training loss: 14.666918
2017-12-25 21:11:09,974 : Iter 350, training loss: 14.658556
2017-12-25 21:11:10,779 : Epoch 3, training loss: 14.652376
2017-12-25 21:11:11,853 : Epoch 3, validation loss: 17.570391
2017-12-25 21:11:12,092 : Training checkpoint has been saved.
2017-12-25 21:11:12,092 : Epoch 4 started:
2017-12-25 21:11:12,300 : Iter 0, training loss: 10.962920
2017-12-25 21:11:14,064 : Iter 10, training loss: 12.019317
2017-12-25 21:11:15,722 : Iter 20, training loss: 12.180103
2017-12-25 21:11:17,621 : Iter 30, training loss: 12.120990
2017-12-25 21:11:19,294 : Iter 40, training loss: 11.939011
2017-12-25 21:11:20,963 : Iter 50, training loss: 12.338465
2017-12-25 21:11:22,800 : Iter 60, training loss: 12.209400
2017-12-25 21:11:24,782 : Iter 70, training loss: 12.257780
2017-12-25 21:11:26,598 : Iter 80, training loss: 12.187346
2017-12-25 21:11:28,470 : Iter 90, training loss: 12.328893
2017-12-25 21:11:30,672 : Iter 100, training loss: 12.380648
2017-12-25 21:11:33,189 : Iter 110, training loss: 12.382485
2017-12-25 21:11:35,540 : Iter 120, training loss: 12.353536
2017-12-25 21:11:37,875 : Iter 130, training loss: 12.394594
2017-12-25 21:11:40,093 : Iter 140, training loss: 12.295431
2017-12-25 21:11:42,170 : Iter 150, training loss: 12.314834
2017-12-25 21:11:44,034 : Iter 160, training loss: 12.370293
2017-12-25 21:11:46,196 : Iter 170, training loss: 12.442331
2017-12-25 21:11:48,122 : Iter 180, training loss: 12.448719
2017-12-25 21:11:50,347 : Iter 190, training loss: 12.495544
2017-12-25 21:11:52,639 : Iter 200, training loss: 12.517277
2017-12-25 21:11:55,080 : Iter 210, training loss: 12.512816
2017-12-25 21:11:57,495 : Iter 220, training loss: 12.585360
2017-12-25 21:11:59,704 : Iter 230, training loss: 12.587536
2017-12-25 21:12:01,608 : Iter 240, training loss: 12.607046
2017-12-25 21:12:03,690 : Iter 250, training loss: 12.707853
2017-12-25 21:12:05,678 : Iter 260, training loss: 12.765015
2017-12-25 21:12:07,683 : Iter 270, training loss: 12.763344
2017-12-25 21:12:09,969 : Iter 280, training loss: 12.771525
2017-12-25 21:12:12,047 : Iter 290, training loss: 12.788577
2017-12-25 21:12:14,142 : Iter 300, training loss: 12.810994
2017-12-25 21:12:16,420 : Iter 310, training loss: 12.837510
2017-12-25 21:12:18,864 : Iter 320, training loss: 12.834365
2017-12-25 21:12:21,170 : Iter 330, training loss: 12.835977
2017-12-25 21:12:23,327 : Iter 340, training loss: 12.825353
2017-12-25 21:12:25,563 : Iter 350, training loss: 12.855699
2017-12-25 21:12:26,745 : Epoch 4, training loss: 12.858598
2017-12-25 21:12:28,427 : Epoch 4, validation loss: 17.839884
2017-12-25 21:12:28,497 : Training checkpoint has been saved.
2017-12-25 21:12:28,497 : Epoch 5 started:
2017-12-25 21:12:28,714 : Iter 0, training loss: 9.412546
2017-12-25 21:12:30,924 : Iter 10, training loss: 10.874058
2017-12-25 21:12:33,152 : Iter 20, training loss: 10.030073
2017-12-25 21:12:35,507 : Iter 30, training loss: 10.328254
2017-12-25 21:12:37,387 : Iter 40, training loss: 10.343266
2017-12-25 21:12:39,425 : Iter 50, training loss: 10.167579
2017-12-25 21:12:41,440 : Iter 60, training loss: 10.379410
2017-12-25 21:12:43,821 : Iter 70, training loss: 10.498335
2017-12-25 21:12:46,160 : Iter 80, training loss: 10.527703
2017-12-25 21:12:48,224 : Iter 90, training loss: 10.668957
2017-12-25 21:12:50,415 : Iter 100, training loss: 10.863945
2017-12-25 21:12:52,784 : Iter 110, training loss: 10.958909
2017-12-25 21:12:55,226 : Iter 120, training loss: 10.951527
2017-12-25 21:12:57,558 : Iter 130, training loss: 11.028045
2017-12-25 21:12:59,848 : Iter 140, training loss: 11.088788
2017-12-25 21:13:01,868 : Iter 150, training loss: 11.210761
2017-12-25 21:13:04,228 : Iter 160, training loss: 11.183944
2017-12-25 21:13:06,070 : Iter 170, training loss: 11.177028
2017-12-25 21:13:08,382 : Iter 180, training loss: 11.219152
2017-12-25 21:13:10,718 : Iter 190, training loss: 11.197313
2017-12-25 21:13:12,975 : Iter 200, training loss: 11.286335
2017-12-25 21:13:15,227 : Iter 210, training loss: 11.262810
2017-12-25 21:13:17,395 : Iter 220, training loss: 11.257026
2017-12-25 21:13:19,781 : Iter 230, training loss: 11.248654
2017-12-25 21:13:21,854 : Iter 240, training loss: 11.312889
2017-12-25 21:13:23,924 : Iter 250, training loss: 11.305838
2017-12-25 21:13:25,952 : Iter 260, training loss: 11.312842
2017-12-25 21:13:28,280 : Iter 270, training loss: 11.340219
2017-12-25 21:13:30,456 : Iter 280, training loss: 11.314289
2017-12-25 21:13:32,600 : Iter 290, training loss: 11.307654
2017-12-25 21:13:34,481 : Iter 300, training loss: 11.327844
2017-12-25 21:13:36,639 : Iter 310, training loss: 11.347298
2017-12-25 21:13:39,222 : Iter 320, training loss: 11.339105
2017-12-25 21:13:41,411 : Iter 330, training loss: 11.376359
2017-12-25 21:13:43,596 : Iter 340, training loss: 11.370699
2017-12-25 21:13:45,739 : Iter 350, training loss: 11.388869
2017-12-25 21:13:46,722 : Epoch 5, training loss: 11.407698
2017-12-25 21:13:48,418 : Epoch 5, validation loss: 16.485005
2017-12-25 21:13:48,555 : Training checkpoint has been saved.
2017-12-25 21:13:48,555 : Epoch 6 started:
2017-12-25 21:13:48,814 : Iter 0, training loss: 9.736299
2017-12-25 21:13:51,215 : Iter 10, training loss: 8.930346
2017-12-25 21:13:53,494 : Iter 20, training loss: 9.164220
2017-12-25 21:13:55,909 : Iter 30, training loss: 9.107560
2017-12-25 21:13:58,034 : Iter 40, training loss: 8.940693
2017-12-25 21:14:00,211 : Iter 50, training loss: 9.169307
2017-12-25 21:14:02,444 : Iter 60, training loss: 9.265949
2017-12-25 21:14:04,670 : Iter 70, training loss: 9.466389
2017-12-25 21:14:06,835 : Iter 80, training loss: 9.502738
2017-12-25 21:14:09,350 : Iter 90, training loss: 9.573294
2017-12-25 21:14:11,661 : Iter 100, training loss: 9.607583
2017-12-25 21:14:13,893 : Iter 110, training loss: 9.603780
2017-12-25 21:14:16,222 : Iter 120, training loss: 9.633555
2017-12-25 21:14:18,598 : Iter 130, training loss: 9.712092
2017-12-25 21:14:20,953 : Iter 140, training loss: 9.804773
2017-12-25 21:14:23,353 : Iter 150, training loss: 9.796378
2017-12-25 21:14:25,646 : Iter 160, training loss: 9.785123
2017-12-25 21:14:27,982 : Iter 170, training loss: 9.840691
2017-12-25 21:14:30,179 : Iter 180, training loss: 9.846979
2017-12-25 21:14:32,197 : Iter 190, training loss: 9.903877
2017-12-25 21:14:34,413 : Iter 200, training loss: 9.914347
2017-12-25 21:14:36,651 : Iter 210, training loss: 9.968096
2017-12-25 21:14:38,788 : Iter 220, training loss: 9.950586
2017-12-25 21:14:40,940 : Iter 230, training loss: 9.977950
2017-12-25 21:14:43,277 : Iter 240, training loss: 10.025126
2017-12-25 21:14:45,482 : Iter 250, training loss: 10.042648
2017-12-25 21:14:47,691 : Iter 260, training loss: 10.031914
2017-12-25 21:14:50,257 : Iter 270, training loss: 10.020169
2017-12-25 21:14:52,301 : Iter 280, training loss: 10.037040
2017-12-25 21:14:54,641 : Iter 290, training loss: 10.068659
2017-12-25 21:14:56,587 : Iter 300, training loss: 10.091018
2017-12-25 21:14:58,968 : Iter 310, training loss: 10.100673
2017-12-25 21:15:01,266 : Iter 320, training loss: 10.103451
2017-12-25 21:15:03,563 : Iter 330, training loss: 10.116457
2017-12-25 21:15:05,919 : Iter 340, training loss: 10.172052
2017-12-25 21:15:08,116 : Iter 350, training loss: 10.210967
2017-12-25 21:15:09,245 : Epoch 6, training loss: 10.224863
2017-12-25 21:15:10,979 : Epoch 6, validation loss: 16.862411
2017-12-25 21:15:11,101 : Training checkpoint has been saved.
2017-12-25 21:15:11,101 : Epoch 7 started:
2017-12-25 21:15:11,418 : Iter 0, training loss: 8.386969
2017-12-25 21:15:13,830 : Iter 10, training loss: 8.601703
2017-12-25 21:15:16,232 : Iter 20, training loss: 8.856437
2017-12-25 21:15:18,517 : Iter 30, training loss: 8.873038
2017-12-25 21:15:20,533 : Iter 40, training loss: 8.905069
2017-12-25 21:15:22,599 : Iter 50, training loss: 8.768081
2017-12-25 21:15:24,812 : Iter 60, training loss: 8.675797
2017-12-25 21:15:26,880 : Iter 70, training loss: 8.698063
2017-12-25 21:15:29,409 : Iter 80, training loss: 8.720640
2017-12-25 21:15:31,699 : Iter 90, training loss: 8.706724
2017-12-25 21:15:33,597 : Iter 100, training loss: 8.724913
2017-12-25 21:15:35,876 : Iter 110, training loss: 8.693658
2017-12-25 21:15:38,293 : Iter 120, training loss: 8.672800
2017-12-25 21:15:40,519 : Iter 130, training loss: 8.721755
2017-12-25 21:15:42,668 : Iter 140, training loss: 8.719982
2017-12-25 21:15:44,605 : Iter 150, training loss: 8.781870
2017-12-25 21:15:46,924 : Iter 160, training loss: 8.808468
2017-12-25 21:15:49,252 : Iter 170, training loss: 8.837300
2017-12-25 21:15:51,484 : Iter 180, training loss: 8.841474
2017-12-25 21:15:53,598 : Iter 190, training loss: 8.823602
2017-12-25 21:15:55,793 : Iter 200, training loss: 8.842486
2017-12-25 21:15:57,951 : Iter 210, training loss: 8.866878
2017-12-25 21:16:00,403 : Iter 220, training loss: 8.905036
2017-12-25 21:16:02,617 : Iter 230, training loss: 8.938032
2017-12-25 21:16:04,668 : Iter 240, training loss: 8.955575
2017-12-25 21:16:06,783 : Iter 250, training loss: 8.954061
2017-12-25 21:16:08,887 : Iter 260, training loss: 8.977252
2017-12-25 21:16:11,314 : Iter 270, training loss: 8.965959
2017-12-25 21:16:13,816 : Iter 280, training loss: 8.987394
2017-12-25 21:16:16,120 : Iter 290, training loss: 9.009167
2017-12-25 21:16:18,033 : Iter 300, training loss: 9.010052
2017-12-25 21:16:20,155 : Iter 310, training loss: 9.058679
2017-12-25 21:16:22,412 : Iter 320, training loss: 9.089267
2017-12-25 21:16:24,719 : Iter 330, training loss: 9.111527
2017-12-25 21:16:27,084 : Iter 340, training loss: 9.135662
2017-12-25 21:16:29,207 : Iter 350, training loss: 9.155367
2017-12-25 21:16:30,265 : Epoch 7, training loss: 9.163130
2017-12-25 21:16:31,755 : Epoch 7, validation loss: 17.220525
2017-12-25 21:16:31,881 : Training checkpoint has been saved.
2017-12-25 21:16:31,881 : Epoch 8 started:
2017-12-25 21:16:32,128 : Iter 0, training loss: 6.641052
2017-12-25 21:16:34,064 : Iter 10, training loss: 7.543842
2017-12-25 21:16:36,172 : Iter 20, training loss: 7.208557
2017-12-25 21:16:38,223 : Iter 30, training loss: 6.974287
2017-12-25 21:16:40,422 : Iter 40, training loss: 7.182232
2017-12-25 21:16:42,873 : Iter 50, training loss: 7.180690
2017-12-25 21:16:45,011 : Iter 60, training loss: 7.318987
2017-12-25 21:16:47,245 : Iter 70, training loss: 7.378544
2017-12-25 21:16:49,299 : Iter 80, training loss: 7.369136
2017-12-25 21:16:51,154 : Iter 90, training loss: 7.392706
2017-12-25 21:16:53,572 : Iter 100, training loss: 7.369652
2017-12-25 21:16:55,914 : Iter 110, training loss: 7.402650
2017-12-25 21:16:58,413 : Iter 120, training loss: 7.407574
2017-12-25 21:17:00,375 : Iter 130, training loss: 7.424335
2017-12-25 21:17:02,463 : Iter 140, training loss: 7.450082
2017-12-25 21:17:04,810 : Iter 150, training loss: 7.467266
2017-12-25 21:17:06,695 : Iter 160, training loss: 7.505387
2017-12-25 21:17:08,892 : Iter 170, training loss: 7.484474
2017-12-25 21:17:11,059 : Iter 180, training loss: 7.534794
2017-12-25 21:17:13,345 : Iter 190, training loss: 7.577899
2017-12-25 21:17:15,654 : Iter 200, training loss: 7.622024
2017-12-25 21:17:17,995 : Iter 210, training loss: 7.646809
2017-12-25 21:17:20,123 : Iter 220, training loss: 7.666809
2017-12-25 21:17:22,459 : Iter 230, training loss: 7.665902
2017-12-25 21:17:24,842 : Iter 240, training loss: 7.697503
2017-12-25 21:17:27,004 : Iter 250, training loss: 7.715526
2017-12-25 21:17:29,166 : Iter 260, training loss: 7.748615
2017-12-25 21:17:31,562 : Iter 270, training loss: 7.748085
2017-12-25 21:17:33,612 : Iter 280, training loss: 7.772400
2017-12-25 21:17:35,482 : Iter 290, training loss: 7.809848
2017-12-25 21:17:37,507 : Iter 300, training loss: 7.831555
2017-12-25 21:17:39,617 : Iter 310, training loss: 7.853873
2017-12-25 21:17:41,985 : Iter 320, training loss: 7.866015
2017-12-25 21:17:44,046 : Iter 330, training loss: 7.877694
2017-12-25 21:17:46,242 : Iter 340, training loss: 7.912179
2017-12-25 21:17:48,619 : Iter 350, training loss: 7.954075
2017-12-25 21:17:50,018 : Epoch 8, training loss: 7.964919
2017-12-25 21:17:51,645 : Epoch 8, validation loss: 18.023937
2017-12-25 21:17:51,764 : Training checkpoint has been saved.
2017-12-25 21:17:51,764 : Epoch 9 started:
2017-12-25 21:17:51,993 : Iter 0, training loss: 5.492306
2017-12-25 21:17:54,070 : Iter 10, training loss: 7.127968
2017-12-25 21:17:56,302 : Iter 20, training loss: 7.124115
2017-12-25 21:17:58,473 : Iter 30, training loss: 7.092048
2017-12-25 21:18:00,676 : Iter 40, training loss: 7.029863
2017-12-25 21:18:02,796 : Iter 50, training loss: 6.927642
2017-12-25 21:18:05,118 : Iter 60, training loss: 6.881298
2017-12-25 21:18:07,260 : Iter 70, training loss: 6.834898
2017-12-25 21:18:09,787 : Iter 80, training loss: 6.777663
2017-12-25 21:18:11,951 : Iter 90, training loss: 6.701271
2017-12-25 21:18:14,356 : Iter 100, training loss: 6.656063
2017-12-25 21:18:16,864 : Iter 110, training loss: 6.734490
2017-12-25 21:18:19,153 : Iter 120, training loss: 6.806437
2017-12-25 21:18:21,573 : Iter 130, training loss: 6.774889
2017-12-25 21:18:23,889 : Iter 140, training loss: 6.811276
2017-12-25 21:18:26,127 : Iter 150, training loss: 6.832120
2017-12-25 21:18:28,442 : Iter 160, training loss: 6.886894
2017-12-25 21:18:30,713 : Iter 170, training loss: 6.909623
2017-12-25 21:18:32,934 : Iter 180, training loss: 6.909481
2017-12-25 21:18:34,961 : Iter 190, training loss: 6.905732
2017-12-25 21:18:37,171 : Iter 200, training loss: 6.908868
2017-12-25 21:18:39,236 : Iter 210, training loss: 6.920327
2017-12-25 21:18:41,557 : Iter 220, training loss: 6.938161
2017-12-25 21:18:43,787 : Iter 230, training loss: 6.963130
2017-12-25 21:18:46,110 : Iter 240, training loss: 6.985703
2017-12-25 21:18:48,469 : Iter 250, training loss: 7.015587
2017-12-25 21:18:50,499 : Iter 260, training loss: 7.006287
2017-12-25 21:18:52,865 : Iter 270, training loss: 7.028480
2017-12-25 21:18:55,418 : Iter 280, training loss: 7.044381
2017-12-25 21:18:57,811 : Iter 290, training loss: 7.049793
2017-12-25 21:19:00,237 : Iter 300, training loss: 7.058858
2017-12-25 21:19:02,181 : Iter 310, training loss: 7.047929
2017-12-25 21:19:04,526 : Iter 320, training loss: 7.053537
2017-12-25 21:19:06,523 : Iter 330, training loss: 7.068167
2017-12-25 21:19:08,970 : Iter 340, training loss: 7.088989
2017-12-25 21:19:11,307 : Iter 350, training loss: 7.081063
2017-12-25 21:19:12,444 : Epoch 9, training loss: 7.084585
2017-12-25 21:19:14,044 : Epoch 9, validation loss: 17.478340
2017-12-25 21:19:14,110 : Training checkpoint has been saved.
2017-12-25 21:19:14,120 : Training process has been finished.
