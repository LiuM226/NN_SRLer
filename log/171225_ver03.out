2017-12-25 21:29:54,041 : Finished building computation graph for training.
2017-12-25 21:29:56,460 : Read in training data successfully.
2017-12-25 21:29:56.484243: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2017-12-25 21:29:57.930144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.873
pciBusID: 0000:03:00.0
totalMemory: 7.92GiB freeMemory: 7.80GiB
2017-12-25 21:29:57.930173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)
2017-12-25 21:29:58,550 : Epoch 0 started:
2017-12-25 21:29:59,119 : Iter 0, training loss: 879.649063
2017-12-25 21:30:01,366 : Iter 10, training loss: 740.482834
2017-12-25 21:30:03,598 : Iter 20, training loss: 832.636775
2017-12-25 21:30:06,001 : Iter 30, training loss: 790.326968
2017-12-25 21:30:08,289 : Iter 40, training loss: 850.752129
2017-12-25 21:30:10,319 : Iter 50, training loss: 821.539203
2017-12-25 21:30:12,520 : Iter 60, training loss: 755.916304
2017-12-25 21:30:14,564 : Iter 70, training loss: 706.188104
2017-12-25 21:30:16,857 : Iter 80, training loss: 646.456448
2017-12-25 21:30:18,986 : Iter 90, training loss: 604.177228
2017-12-25 21:30:21,269 : Iter 100, training loss: 562.890896
2017-12-25 21:30:23,635 : Iter 110, training loss: 529.904236
2017-12-25 21:30:25,805 : Iter 120, training loss: 497.568542
2017-12-25 21:30:28,254 : Iter 130, training loss: 470.704599
2017-12-25 21:30:30,348 : Iter 140, training loss: 446.749363
2017-12-25 21:30:32,849 : Iter 150, training loss: 424.942084
2017-12-25 21:30:35,087 : Iter 160, training loss: 404.080889
2017-12-25 21:30:37,494 : Iter 170, training loss: 385.709668
2017-12-25 21:30:40,051 : Iter 180, training loss: 370.810749
2017-12-25 21:30:42,113 : Iter 190, training loss: 356.452454
2017-12-25 21:30:44,217 : Iter 200, training loss: 344.418653
2017-12-25 21:30:46,618 : Iter 210, training loss: 332.563887
2017-12-25 21:30:48,774 : Iter 220, training loss: 320.740650
2017-12-25 21:30:51,073 : Iter 230, training loss: 310.434719
2017-12-25 21:30:53,350 : Iter 240, training loss: 301.685959
2017-12-25 21:30:55,452 : Iter 250, training loss: 292.590615
2017-12-25 21:30:57,892 : Iter 260, training loss: 284.551381
2017-12-25 21:30:59,949 : Iter 270, training loss: 277.711563
2017-12-25 21:31:02,466 : Iter 280, training loss: 270.287111
2017-12-25 21:31:04,818 : Iter 290, training loss: 263.614167
2017-12-25 21:31:06,691 : Iter 300, training loss: 257.534339
2017-12-25 21:31:08,952 : Iter 310, training loss: 252.292435
2017-12-25 21:31:11,107 : Iter 320, training loss: 246.822782
2017-12-25 21:31:13,260 : Iter 330, training loss: 241.372191
2017-12-25 21:31:15,553 : Iter 340, training loss: 236.161971
2017-12-25 21:31:17,954 : Iter 350, training loss: 232.029944
2017-12-25 21:31:18,949 : Epoch 0, training loss: 229.731287
2017-12-25 21:31:20,907 : Epoch 0, validation loss: 78.116927
2017-12-25 21:31:21,098 : Training checkpoint has been saved.
2017-12-25 21:31:21,099 : Epoch 1 started:
2017-12-25 21:31:21,270 : Iter 0, training loss: 85.249629
2017-12-25 21:31:23,753 : Iter 10, training loss: 66.207864
2017-12-25 21:31:26,176 : Iter 20, training loss: 65.180138
2017-12-25 21:31:28,354 : Iter 30, training loss: 65.561609
2017-12-25 21:31:30,870 : Iter 40, training loss: 64.102147
2017-12-25 21:31:33,288 : Iter 50, training loss: 68.298177
2017-12-25 21:31:35,603 : Iter 60, training loss: 66.096689
2017-12-25 21:31:37,777 : Iter 70, training loss: 65.112161
2017-12-25 21:31:40,039 : Iter 80, training loss: 65.059674
2017-12-25 21:31:42,362 : Iter 90, training loss: 64.858023
2017-12-25 21:31:44,742 : Iter 100, training loss: 63.408263
2017-12-25 21:31:46,871 : Iter 110, training loss: 65.695968
2017-12-25 21:31:49,131 : Iter 120, training loss: 65.426640
2017-12-25 21:31:51,113 : Iter 130, training loss: 64.640319
2017-12-25 21:31:53,435 : Iter 140, training loss: 63.970103
2017-12-25 21:31:55,559 : Iter 150, training loss: 64.392242
2017-12-25 21:31:57,666 : Iter 160, training loss: 63.703938
2017-12-25 21:32:00,011 : Iter 170, training loss: 63.432169
2017-12-25 21:32:02,152 : Iter 180, training loss: 62.689524
2017-12-25 21:32:04,510 : Iter 190, training loss: 62.039786
2017-12-25 21:32:06,896 : Iter 200, training loss: 61.635788
2017-12-25 21:32:09,026 : Iter 210, training loss: 60.681034
2017-12-25 21:32:11,513 : Iter 220, training loss: 62.055735
2017-12-25 21:32:13,965 : Iter 230, training loss: 62.213070
2017-12-25 21:32:16,439 : Iter 240, training loss: 62.003240
2017-12-25 21:32:18,755 : Iter 250, training loss: 61.234650
2017-12-25 21:32:21,030 : Iter 260, training loss: 61.436404
2017-12-25 21:32:22,979 : Iter 270, training loss: 60.714326
2017-12-25 21:32:25,153 : Iter 280, training loss: 60.077301
2017-12-25 21:32:27,226 : Iter 290, training loss: 59.138121
2017-12-25 21:32:29,769 : Iter 300, training loss: 58.638497
2017-12-25 21:32:32,040 : Iter 310, training loss: 57.943622
2017-12-25 21:32:34,296 : Iter 320, training loss: 57.547927
2017-12-25 21:32:36,520 : Iter 330, training loss: 57.480703
2017-12-25 21:32:38,822 : Iter 340, training loss: 57.193193
2017-12-25 21:32:41,080 : Iter 350, training loss: 56.906374
2017-12-25 21:32:42,056 : Epoch 1, training loss: 56.716846
2017-12-25 21:32:43,579 : Epoch 1, validation loss: 34.877710
2017-12-25 21:32:43,700 : Training checkpoint has been saved.
2017-12-25 21:32:43,700 : Epoch 2 started:
2017-12-25 21:32:43,877 : Iter 0, training loss: 26.565767
2017-12-25 21:32:46,581 : Iter 10, training loss: 39.013522
2017-12-25 21:32:48,925 : Iter 20, training loss: 42.914846
2017-12-25 21:32:51,161 : Iter 30, training loss: 39.343596
2017-12-25 21:32:53,289 : Iter 40, training loss: 38.013875
2017-12-25 21:32:55,339 : Iter 50, training loss: 37.645171
2017-12-25 21:32:57,581 : Iter 60, training loss: 37.068001
2017-12-25 21:33:00,093 : Iter 70, training loss: 38.046288
2017-12-25 21:33:02,249 : Iter 80, training loss: 38.254763
2017-12-25 21:33:04,727 : Iter 90, training loss: 38.827683
2017-12-25 21:33:07,137 : Iter 100, training loss: 37.592704
2017-12-25 21:33:09,794 : Iter 110, training loss: 38.521289
2017-12-25 21:33:12,200 : Iter 120, training loss: 39.026814
2017-12-25 21:33:14,587 : Iter 130, training loss: 38.745945
2017-12-25 21:33:17,086 : Iter 140, training loss: 38.432261
2017-12-25 21:33:19,368 : Iter 150, training loss: 38.419444
2017-12-25 21:33:21,924 : Iter 160, training loss: 38.993453
2017-12-25 21:33:24,537 : Iter 170, training loss: 38.773353
2017-12-25 21:33:26,849 : Iter 180, training loss: 38.748317
2017-12-25 21:33:28,967 : Iter 190, training loss: 38.037112
2017-12-25 21:33:31,093 : Iter 200, training loss: 37.788194
2017-12-25 21:33:33,550 : Iter 210, training loss: 37.488481
2017-12-25 21:33:35,928 : Iter 220, training loss: 37.812439
2017-12-25 21:33:38,297 : Iter 230, training loss: 37.686578
2017-12-25 21:33:40,893 : Iter 240, training loss: 37.867846
2017-12-25 21:33:43,250 : Iter 250, training loss: 37.652710
2017-12-25 21:33:45,961 : Iter 260, training loss: 38.056891
2017-12-25 21:33:48,301 : Iter 270, training loss: 38.108137
2017-12-25 21:33:50,556 : Iter 280, training loss: 37.753979
2017-12-25 21:33:52,763 : Iter 290, training loss: 37.573906
2017-12-25 21:33:54,894 : Iter 300, training loss: 37.392391
2017-12-25 21:33:57,442 : Iter 310, training loss: 37.278847
2017-12-25 21:33:59,886 : Iter 320, training loss: 37.046232
2017-12-25 21:34:02,157 : Iter 330, training loss: 36.886722
2017-12-25 21:34:04,242 : Iter 340, training loss: 36.808900
2017-12-25 21:34:06,356 : Iter 350, training loss: 36.633476
2017-12-25 21:34:07,485 : Epoch 2, training loss: 36.567169
2017-12-25 21:34:08,998 : Epoch 2, validation loss: 46.696337
2017-12-25 21:34:09,314 : Training checkpoint has been saved.
2017-12-25 21:34:09,314 : Epoch 3 started:
2017-12-25 21:34:09,519 : Iter 0, training loss: 34.366191
2017-12-25 21:34:11,607 : Iter 10, training loss: 27.342666
2017-12-25 21:34:13,921 : Iter 20, training loss: 27.367380
2017-12-25 21:34:16,419 : Iter 30, training loss: 27.180899
2017-12-25 21:34:18,729 : Iter 40, training loss: 27.931540
2017-12-25 21:34:20,962 : Iter 50, training loss: 28.137978
2017-12-25 21:34:23,334 : Iter 60, training loss: 28.555377
2017-12-25 21:34:25,422 : Iter 70, training loss: 28.772553
2017-12-25 21:34:27,952 : Iter 80, training loss: 29.407755
2017-12-25 21:34:30,187 : Iter 90, training loss: 29.259225
2017-12-25 21:34:32,544 : Iter 100, training loss: 29.114496
2017-12-25 21:34:34,727 : Iter 110, training loss: 29.760881
2017-12-25 21:34:36,907 : Iter 120, training loss: 29.403656
2017-12-25 21:34:39,314 : Iter 130, training loss: 29.304139
2017-12-25 21:34:41,786 : Iter 140, training loss: 28.946778
2017-12-25 21:34:44,190 : Iter 150, training loss: 28.672956
2017-12-25 21:34:46,351 : Iter 160, training loss: 29.152398
2017-12-25 21:34:48,875 : Iter 170, training loss: 29.224863
2017-12-25 21:34:51,259 : Iter 180, training loss: 29.490201
2017-12-25 21:34:53,362 : Iter 190, training loss: 29.636362
2017-12-25 21:34:55,569 : Iter 200, training loss: 29.818612
2017-12-25 21:34:57,947 : Iter 210, training loss: 29.646260
2017-12-25 21:35:00,117 : Iter 220, training loss: 29.511593
2017-12-25 21:35:02,859 : Iter 230, training loss: 29.586851
2017-12-25 21:35:05,197 : Iter 240, training loss: 29.375996
2017-12-25 21:35:07,654 : Iter 250, training loss: 29.491663
2017-12-25 21:35:09,942 : Iter 260, training loss: 29.315656
2017-12-25 21:35:12,535 : Iter 270, training loss: 29.849894
2017-12-25 21:35:14,700 : Iter 280, training loss: 29.759751
2017-12-25 21:35:17,107 : Iter 290, training loss: 29.611392
2017-12-25 21:35:19,417 : Iter 300, training loss: 29.941924
2017-12-25 21:35:21,821 : Iter 310, training loss: 29.801295
2017-12-25 21:35:23,989 : Iter 320, training loss: 29.804319
2017-12-25 21:35:26,440 : Iter 330, training loss: 29.862356
2017-12-25 21:35:28,721 : Iter 340, training loss: 29.857342
2017-12-25 21:35:31,180 : Iter 350, training loss: 29.761203
2017-12-25 21:35:32,564 : Epoch 3, training loss: 29.765693
2017-12-25 21:35:34,178 : Epoch 3, validation loss: 29.611849
2017-12-25 21:35:34,316 : Training checkpoint has been saved.
2017-12-25 21:35:34,316 : Epoch 4 started:
2017-12-25 21:35:34,559 : Iter 0, training loss: 18.317800
2017-12-25 21:35:36,915 : Iter 10, training loss: 22.829327
2017-12-25 21:35:39,119 : Iter 20, training loss: 21.650410
2017-12-25 21:35:41,386 : Iter 30, training loss: 21.514321
2017-12-25 21:35:43,508 : Iter 40, training loss: 21.644478
2017-12-25 21:35:45,914 : Iter 50, training loss: 23.037892
2017-12-25 21:35:48,022 : Iter 60, training loss: 22.497425
2017-12-25 21:35:50,209 : Iter 70, training loss: 22.946217
2017-12-25 21:35:52,239 : Iter 80, training loss: 22.932126
2017-12-25 21:35:54,207 : Iter 90, training loss: 23.579751
2017-12-25 21:35:56,301 : Iter 100, training loss: 23.754584
2017-12-25 21:35:58,477 : Iter 110, training loss: 23.886401
2017-12-25 21:36:00,945 : Iter 120, training loss: 23.846128
2017-12-25 21:36:03,084 : Iter 130, training loss: 23.622493
2017-12-25 21:36:05,285 : Iter 140, training loss: 23.488533
2017-12-25 21:36:07,699 : Iter 150, training loss: 23.540290
2017-12-25 21:36:09,953 : Iter 160, training loss: 23.547367
2017-12-25 21:36:12,000 : Iter 170, training loss: 24.317105
2017-12-25 21:36:14,357 : Iter 180, training loss: 24.139897
2017-12-25 21:36:16,858 : Iter 190, training loss: 23.978718
2017-12-25 21:36:19,262 : Iter 200, training loss: 23.913989
2017-12-25 21:36:21,660 : Iter 210, training loss: 23.816943
2017-12-25 21:36:23,808 : Iter 220, training loss: 24.109314
2017-12-25 21:36:25,898 : Iter 230, training loss: 24.028266
2017-12-25 21:36:28,270 : Iter 240, training loss: 24.255738
2017-12-25 21:36:30,272 : Iter 250, training loss: 24.445229
2017-12-25 21:36:32,635 : Iter 260, training loss: 24.769678
2017-12-25 21:36:34,803 : Iter 270, training loss: 24.602164
2017-12-25 21:36:37,058 : Iter 280, training loss: 24.615258
2017-12-25 21:36:39,242 : Iter 290, training loss: 24.565691
2017-12-25 21:36:41,263 : Iter 300, training loss: 24.494411
2017-12-25 21:36:43,681 : Iter 310, training loss: 24.557161
2017-12-25 21:36:45,797 : Iter 320, training loss: 24.518863
2017-12-25 21:36:47,879 : Iter 330, training loss: 24.424934
2017-12-25 21:36:50,102 : Iter 340, training loss: 24.290613
2017-12-25 21:36:52,353 : Iter 350, training loss: 24.221946
2017-12-25 21:36:53,463 : Epoch 4, training loss: 24.246941
2017-12-25 21:36:55,219 : Epoch 4, validation loss: 30.422284
2017-12-25 21:36:55,300 : Training checkpoint has been saved.
2017-12-25 21:36:55,300 : Epoch 5 started:
2017-12-25 21:36:55,487 : Iter 0, training loss: 18.107229
2017-12-25 21:36:57,659 : Iter 10, training loss: 25.652391
2017-12-25 21:36:59,842 : Iter 20, training loss: 21.350677
2017-12-25 21:37:02,365 : Iter 30, training loss: 20.688366
2017-12-25 21:37:04,903 : Iter 40, training loss: 23.824499
2017-12-25 21:37:07,179 : Iter 50, training loss: 22.595476
2017-12-25 21:37:09,296 : Iter 60, training loss: 22.574934
2017-12-25 21:37:11,462 : Iter 70, training loss: 22.075541
2017-12-25 21:37:13,333 : Iter 80, training loss: 22.244382
2017-12-25 21:37:15,769 : Iter 90, training loss: 22.462372
2017-12-25 21:37:17,899 : Iter 100, training loss: 22.296008
2017-12-25 21:37:20,256 : Iter 110, training loss: 22.576184
2017-12-25 21:37:22,616 : Iter 120, training loss: 22.474249
2017-12-25 21:37:24,982 : Iter 130, training loss: 22.336725
2017-12-25 21:37:27,180 : Iter 140, training loss: 22.122646
2017-12-25 21:37:29,476 : Iter 150, training loss: 22.087825
2017-12-25 21:37:31,614 : Iter 160, training loss: 22.067258
2017-12-25 21:37:33,778 : Iter 170, training loss: 21.891409
2017-12-25 21:37:36,075 : Iter 180, training loss: 21.785162
2017-12-25 21:37:38,656 : Iter 190, training loss: 21.785415
2017-12-25 21:37:40,984 : Iter 200, training loss: 21.840432
2017-12-25 21:37:43,094 : Iter 210, training loss: 21.780353
2017-12-25 21:37:45,213 : Iter 220, training loss: 21.640419
2017-12-25 21:37:47,402 : Iter 230, training loss: 21.830948
2017-12-25 21:37:49,669 : Iter 240, training loss: 21.769650
2017-12-25 21:37:51,880 : Iter 250, training loss: 21.612555
2017-12-25 21:37:54,258 : Iter 260, training loss: 21.454374
2017-12-25 21:37:56,688 : Iter 270, training loss: 21.435186
2017-12-25 21:37:58,830 : Iter 280, training loss: 21.312546
2017-12-25 21:38:01,391 : Iter 290, training loss: 21.371523
2017-12-25 21:38:03,614 : Iter 300, training loss: 21.274428
2017-12-25 21:38:05,976 : Iter 310, training loss: 21.220162
2017-12-25 21:38:08,267 : Iter 320, training loss: 21.153250
2017-12-25 21:38:10,636 : Iter 330, training loss: 21.094928
2017-12-25 21:38:12,790 : Iter 340, training loss: 21.032433
2017-12-25 21:38:15,072 : Iter 350, training loss: 21.141874
2017-12-25 21:38:16,341 : Epoch 5, training loss: 21.106358
2017-12-25 21:38:18,183 : Epoch 5, validation loss: 26.305595
2017-12-25 21:38:18,309 : Training checkpoint has been saved.
2017-12-25 21:38:18,309 : Epoch 6 started:
2017-12-25 21:38:18,542 : Iter 0, training loss: 20.629478
2017-12-25 21:38:20,768 : Iter 10, training loss: 18.800866
2017-12-25 21:38:23,112 : Iter 20, training loss: 18.290824
2017-12-25 21:38:25,605 : Iter 30, training loss: 17.399213
2017-12-25 21:38:28,054 : Iter 40, training loss: 16.631063
2017-12-25 21:38:30,232 : Iter 50, training loss: 16.827494
2017-12-25 21:38:32,594 : Iter 60, training loss: 16.882867
2017-12-25 21:38:34,722 : Iter 70, training loss: 17.470920
2017-12-25 21:38:37,281 : Iter 80, training loss: 17.626471
2017-12-25 21:38:39,490 : Iter 90, training loss: 18.284473
2017-12-25 21:38:41,856 : Iter 100, training loss: 18.077934
2017-12-25 21:38:44,343 : Iter 110, training loss: 18.007760
2017-12-25 21:38:46,669 : Iter 120, training loss: 18.059218
2017-12-25 21:38:49,098 : Iter 130, training loss: 18.038715
2017-12-25 21:38:51,812 : Iter 140, training loss: 18.293904
2017-12-25 21:38:54,048 : Iter 150, training loss: 18.361004
2017-12-25 21:38:56,056 : Iter 160, training loss: 18.192347
2017-12-25 21:38:58,438 : Iter 170, training loss: 18.565228
2017-12-25 21:39:00,705 : Iter 180, training loss: 18.862394
2017-12-25 21:39:02,890 : Iter 190, training loss: 18.844543
2017-12-25 21:39:04,811 : Iter 200, training loss: 18.983341
2017-12-25 21:39:06,931 : Iter 210, training loss: 19.035841
2017-12-25 21:39:08,768 : Iter 220, training loss: 18.883531
2017-12-25 21:39:11,054 : Iter 230, training loss: 18.851800
2017-12-25 21:39:13,332 : Iter 240, training loss: 18.777955
2017-12-25 21:39:15,651 : Iter 250, training loss: 18.715147
2017-12-25 21:39:17,940 : Iter 260, training loss: 18.586105
2017-12-25 21:39:20,253 : Iter 270, training loss: 18.519808
2017-12-25 21:39:22,418 : Iter 280, training loss: 18.454378
2017-12-25 21:39:24,337 : Iter 290, training loss: 18.479444
2017-12-25 21:39:26,307 : Iter 300, training loss: 18.490767
2017-12-25 21:39:28,158 : Iter 310, training loss: 18.401399
2017-12-25 21:39:30,107 : Iter 320, training loss: 18.323256
2017-12-25 21:39:32,032 : Iter 330, training loss: 18.274450
2017-12-25 21:39:33,919 : Iter 340, training loss: 18.326542
2017-12-25 21:39:35,950 : Iter 350, training loss: 18.323090
2017-12-25 21:39:37,019 : Epoch 6, training loss: 18.293532
2017-12-25 21:39:38,097 : Epoch 6, validation loss: 24.479298
2017-12-25 21:39:38,168 : Training checkpoint has been saved.
2017-12-25 21:39:38,168 : Epoch 7 started:
2017-12-25 21:39:38,325 : Iter 0, training loss: 13.935610
2017-12-25 21:39:40,298 : Iter 10, training loss: 14.975896
2017-12-25 21:39:42,082 : Iter 20, training loss: 15.572449
2017-12-25 21:39:44,032 : Iter 30, training loss: 15.326746
2017-12-25 21:39:45,921 : Iter 40, training loss: 15.600097
2017-12-25 21:39:47,846 : Iter 50, training loss: 15.179001
2017-12-25 21:39:49,873 : Iter 60, training loss: 15.094798
2017-12-25 21:39:51,730 : Iter 70, training loss: 15.127097
2017-12-25 21:39:53,600 : Iter 80, training loss: 15.067318
2017-12-25 21:39:55,582 : Iter 90, training loss: 15.369699
2017-12-25 21:39:57,405 : Iter 100, training loss: 15.307219
2017-12-25 21:39:59,298 : Iter 110, training loss: 15.152942
2017-12-25 21:40:01,310 : Iter 120, training loss: 15.020988
2017-12-25 21:40:03,132 : Iter 130, training loss: 14.962709
2017-12-25 21:40:05,072 : Iter 140, training loss: 14.942225
2017-12-25 21:40:06,961 : Iter 150, training loss: 14.925010
2017-12-25 21:40:08,869 : Iter 160, training loss: 15.066714
2017-12-25 21:40:10,823 : Iter 170, training loss: 15.225884
2017-12-25 21:40:12,614 : Iter 180, training loss: 15.226321
2017-12-25 21:40:14,423 : Iter 190, training loss: 15.199199
2017-12-25 21:40:16,216 : Iter 200, training loss: 15.109890
2017-12-25 21:40:18,276 : Iter 210, training loss: 15.047520
2017-12-25 21:40:20,264 : Iter 220, training loss: 15.275592
2017-12-25 21:40:22,098 : Iter 230, training loss: 15.269727
2017-12-25 21:40:24,083 : Iter 240, training loss: 15.210307
2017-12-25 21:40:25,918 : Iter 250, training loss: 15.236277
2017-12-25 21:40:27,994 : Iter 260, training loss: 15.292238
2017-12-25 21:40:29,794 : Iter 270, training loss: 15.261058
2017-12-25 21:40:31,675 : Iter 280, training loss: 15.315500
2017-12-25 21:40:33,541 : Iter 290, training loss: 15.305392
2017-12-25 21:40:35,563 : Iter 300, training loss: 15.246447
2017-12-25 21:40:37,438 : Iter 310, training loss: 15.345895
2017-12-25 21:40:39,409 : Iter 320, training loss: 15.335080
2017-12-25 21:40:41,338 : Iter 330, training loss: 15.373392
2017-12-25 21:40:43,285 : Iter 340, training loss: 15.434096
2017-12-25 21:40:45,191 : Iter 350, training loss: 15.487639
2017-12-25 21:40:46,004 : Epoch 7, training loss: 15.494566
2017-12-25 21:40:47,144 : Epoch 7, validation loss: 24.246812
2017-12-25 21:40:47,214 : Training checkpoint has been saved.
2017-12-25 21:40:47,214 : Epoch 8 started:
2017-12-25 21:40:47,451 : Iter 0, training loss: 12.553396
2017-12-25 21:40:49,344 : Iter 10, training loss: 12.519223
2017-12-25 21:40:51,185 : Iter 20, training loss: 12.028626
2017-12-25 21:40:52,970 : Iter 30, training loss: 11.865541
2017-12-25 21:40:54,860 : Iter 40, training loss: 12.244617
2017-12-25 21:40:56,608 : Iter 50, training loss: 12.315681
2017-12-25 21:40:58,507 : Iter 60, training loss: 12.375107
2017-12-25 21:41:00,528 : Iter 70, training loss: 12.333574
2017-12-25 21:41:02,392 : Iter 80, training loss: 12.346996
2017-12-25 21:41:04,142 : Iter 90, training loss: 12.405112
2017-12-25 21:41:05,855 : Iter 100, training loss: 12.394253
2017-12-25 21:41:07,589 : Iter 110, training loss: 12.369503
2017-12-25 21:41:09,555 : Iter 120, training loss: 12.386983
2017-12-25 21:41:11,377 : Iter 130, training loss: 12.345283
2017-12-25 21:41:13,195 : Iter 140, training loss: 12.478204
2017-12-25 21:41:15,146 : Iter 150, training loss: 12.536941
2017-12-25 21:41:17,000 : Iter 160, training loss: 12.714075
2017-12-25 21:41:18,956 : Iter 170, training loss: 12.673587
2017-12-25 21:41:20,819 : Iter 180, training loss: 12.717679
2017-12-25 21:41:22,967 : Iter 190, training loss: 12.788116
2017-12-25 21:41:24,881 : Iter 200, training loss: 12.831239
2017-12-25 21:41:26,768 : Iter 210, training loss: 12.820245
2017-12-25 21:41:28,713 : Iter 220, training loss: 12.854409
2017-12-25 21:41:30,497 : Iter 230, training loss: 13.024649
2017-12-25 21:41:32,340 : Iter 240, training loss: 13.055220
2017-12-25 21:41:34,185 : Iter 250, training loss: 13.027670
2017-12-25 21:41:36,008 : Iter 260, training loss: 13.032947
2017-12-25 21:41:37,898 : Iter 270, training loss: 13.014889
2017-12-25 21:41:39,864 : Iter 280, training loss: 13.088295
2017-12-25 21:41:41,868 : Iter 290, training loss: 13.126579
2017-12-25 21:41:43,660 : Iter 300, training loss: 13.104495
2017-12-25 21:41:45,539 : Iter 310, training loss: 13.109317
2017-12-25 21:41:47,396 : Iter 320, training loss: 13.189899
2017-12-25 21:41:49,166 : Iter 330, training loss: 13.221324
2017-12-25 21:41:51,056 : Iter 340, training loss: 13.253645
2017-12-25 21:41:53,243 : Iter 350, training loss: 13.281069
2017-12-25 21:41:54,299 : Epoch 8, training loss: 13.292162
2017-12-25 21:41:55,524 : Epoch 8, validation loss: 25.375477
2017-12-25 21:41:55,600 : Training checkpoint has been saved.
2017-12-25 21:41:55,600 : Epoch 9 started:
2017-12-25 21:41:55,766 : Iter 0, training loss: 9.638743
2017-12-25 21:41:57,689 : Iter 10, training loss: 11.732383
2017-12-25 21:41:59,657 : Iter 20, training loss: 11.862961
2017-12-25 21:42:01,465 : Iter 30, training loss: 11.547896
2017-12-25 21:42:03,319 : Iter 40, training loss: 11.378052
2017-12-25 21:42:05,492 : Iter 50, training loss: 11.350294
2017-12-25 21:42:07,201 : Iter 60, training loss: 11.297185
2017-12-25 21:42:09,097 : Iter 70, training loss: 11.226422
2017-12-25 21:42:10,810 : Iter 80, training loss: 11.220263
2017-12-25 21:42:12,664 : Iter 90, training loss: 11.187100
2017-12-25 21:42:14,598 : Iter 100, training loss: 11.112060
2017-12-25 21:42:16,385 : Iter 110, training loss: 11.198844
2017-12-25 21:42:18,043 : Iter 120, training loss: 11.310144
2017-12-25 21:42:20,012 : Iter 130, training loss: 11.300197
2017-12-25 21:42:22,077 : Iter 140, training loss: 11.292287
2017-12-25 21:42:23,861 : Iter 150, training loss: 11.290399
2017-12-25 21:42:25,707 : Iter 160, training loss: 11.317124
2017-12-25 21:42:27,750 : Iter 170, training loss: 11.373360
2017-12-25 21:42:29,558 : Iter 180, training loss: 11.432175
2017-12-25 21:42:31,520 : Iter 190, training loss: 11.399582
2017-12-25 21:42:33,370 : Iter 200, training loss: 11.413965
2017-12-25 21:42:35,097 : Iter 210, training loss: 11.419376
2017-12-25 21:42:36,967 : Iter 220, training loss: 11.412203
2017-12-25 21:42:38,784 : Iter 230, training loss: 11.412487
2017-12-25 21:42:40,725 : Iter 240, training loss: 11.436054
2017-12-25 21:42:42,549 : Iter 250, training loss: 11.478169
2017-12-25 21:42:44,500 : Iter 260, training loss: 11.475484
2017-12-25 21:42:46,550 : Iter 270, training loss: 11.459213
2017-12-25 21:42:48,416 : Iter 280, training loss: 11.448185
2017-12-25 21:42:50,366 : Iter 290, training loss: 11.430864
2017-12-25 21:42:52,417 : Iter 300, training loss: 11.403930
2017-12-25 21:42:54,241 : Iter 310, training loss: 11.370140
2017-12-25 21:42:56,122 : Iter 320, training loss: 11.349317
2017-12-25 21:42:57,946 : Iter 330, training loss: 11.355569
2017-12-25 21:42:59,598 : Iter 340, training loss: 11.390379
2017-12-25 21:43:01,442 : Iter 350, training loss: 11.381351
2017-12-25 21:43:02,270 : Epoch 9, training loss: 11.397892
2017-12-25 21:43:03,361 : Epoch 9, validation loss: 24.929134
2017-12-25 21:43:03,445 : Training checkpoint has been saved.
2017-12-25 21:43:03,457 : Training process has been finished.
