/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
2017-12-25 20:50:31,632 : Finished building computation graph for training.
2017-12-25 20:50:34,159 : Read in training data successfully.
2017-12-25 20:50:34.180178: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2017-12-25 20:50:34.911730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.873
pciBusID: 0000:03:00.0
totalMemory: 7.92GiB freeMemory: 7.80GiB
2017-12-25 20:50:34.911774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)
2017-12-25 20:50:35,599 : Epoch 0 started:
2017-12-25 20:50:36,153 : Iter 0, training loss: 332.448516
2017-12-25 20:50:38,729 : Iter 10, training loss: 211.244558
2017-12-25 20:50:41,214 : Iter 20, training loss: 132.547759
2017-12-25 20:50:43,690 : Iter 30, training loss: 99.739431
2017-12-25 20:50:46,146 : Iter 40, training loss: 83.777929
2017-12-25 20:50:48,619 : Iter 50, training loss: 72.339056
2017-12-25 20:50:51,173 : Iter 60, training loss: 64.388167
2017-12-25 20:50:53,765 : Iter 70, training loss: 58.921189
2017-12-25 20:50:56,393 : Iter 80, training loss: 54.089064
2017-12-25 20:50:59,019 : Iter 90, training loss: 51.076996
2017-12-25 20:51:01,571 : Iter 100, training loss: 47.986205
2017-12-25 20:51:03,996 : Iter 110, training loss: 45.780891
2017-12-25 20:51:06,551 : Iter 120, training loss: 43.385986
2017-12-25 20:51:09,024 : Iter 130, training loss: 41.615692
2017-12-25 20:51:11,492 : Iter 140, training loss: 40.077716
2017-12-25 20:51:13,788 : Iter 150, training loss: 38.448424
2017-12-25 20:51:16,317 : Iter 160, training loss: 36.990573
2017-12-25 20:51:18,740 : Iter 170, training loss: 35.807192
2017-12-25 20:51:21,321 : Iter 180, training loss: 34.844953
2017-12-25 20:51:23,924 : Iter 190, training loss: 33.911688
2017-12-25 20:51:26,464 : Iter 200, training loss: 33.012233
2017-12-25 20:51:29,118 : Iter 210, training loss: 32.196865
2017-12-25 20:51:31,600 : Iter 220, training loss: 31.293108
2017-12-25 20:51:34,070 : Iter 230, training loss: 30.600824
2017-12-25 20:51:36,491 : Iter 240, training loss: 29.942301
2017-12-25 20:51:39,004 : Iter 250, training loss: 29.353006
2017-12-25 20:51:41,546 : Iter 260, training loss: 28.809829
2017-12-25 20:51:44,209 : Iter 270, training loss: 28.385661
2017-12-25 20:51:46,635 : Iter 280, training loss: 27.832597
2017-12-25 20:51:49,084 : Iter 290, training loss: 27.316295
2017-12-25 20:51:51,506 : Iter 300, training loss: 26.886483
2017-12-25 20:51:54,092 : Iter 310, training loss: 26.505414
2017-12-25 20:51:56,752 : Iter 320, training loss: 26.108930
2017-12-25 20:51:59,213 : Iter 330, training loss: 25.648692
2017-12-25 20:52:01,765 : Iter 340, training loss: 25.255419
2017-12-25 20:52:04,116 : Iter 350, training loss: 24.820105
2017-12-25 20:52:05,475 : Epoch 0, training loss: 24.638725
2017-12-25 20:52:07,178 : Epoch 0, validation loss: 18.399870
2017-12-25 20:52:07,288 : Training checkpoint has been saved.
2017-12-25 20:52:07,288 : Epoch 1 started:
2017-12-25 20:52:07,522 : Iter 0, training loss: 17.928978
2017-12-25 20:52:10,287 : Iter 10, training loss: 11.227180
2017-12-25 20:52:13,037 : Iter 20, training loss: 10.590804
2017-12-25 20:52:15,428 : Iter 30, training loss: 11.147335
2017-12-25 20:52:17,933 : Iter 40, training loss: 10.955042
2017-12-25 20:52:20,511 : Iter 50, training loss: 11.079479
2017-12-25 20:52:23,163 : Iter 60, training loss: 10.803327
2017-12-25 20:52:25,689 : Iter 70, training loss: 10.959580
2017-12-25 20:52:28,146 : Iter 80, training loss: 10.954873
2017-12-25 20:52:30,853 : Iter 90, training loss: 10.886902
2017-12-25 20:52:33,450 : Iter 100, training loss: 10.836641
2017-12-25 20:52:35,933 : Iter 110, training loss: 10.905445
2017-12-25 20:52:38,444 : Iter 120, training loss: 11.004345
2017-12-25 20:52:40,777 : Iter 130, training loss: 10.869569
2017-12-25 20:52:43,326 : Iter 140, training loss: 10.805257
2017-12-25 20:52:45,763 : Iter 150, training loss: 10.844635
2017-12-25 20:52:48,174 : Iter 160, training loss: 10.747412
2017-12-25 20:52:50,624 : Iter 170, training loss: 10.772867
2017-12-25 20:52:53,060 : Iter 180, training loss: 10.708944
2017-12-25 20:52:55,600 : Iter 190, training loss: 10.642195
2017-12-25 20:52:58,070 : Iter 200, training loss: 10.654210
2017-12-25 20:53:00,702 : Iter 210, training loss: 10.554858
2017-12-25 20:53:03,162 : Iter 220, training loss: 10.744939
2017-12-25 20:53:05,474 : Iter 230, training loss: 10.630982
2017-12-25 20:53:07,948 : Iter 240, training loss: 10.571500
2017-12-25 20:53:10,434 : Iter 250, training loss: 10.510415
2017-12-25 20:53:12,998 : Iter 260, training loss: 10.460642
2017-12-25 20:53:15,642 : Iter 270, training loss: 10.369054
2017-12-25 20:53:18,262 : Iter 280, training loss: 10.290194
2017-12-25 20:53:20,914 : Iter 290, training loss: 10.207885
2017-12-25 20:53:23,470 : Iter 300, training loss: 10.135470
2017-12-25 20:53:26,076 : Iter 310, training loss: 10.077627
2017-12-25 20:53:28,604 : Iter 320, training loss: 10.035891
2017-12-25 20:53:30,987 : Iter 330, training loss: 9.977224
2017-12-25 20:53:33,435 : Iter 340, training loss: 9.903560
2017-12-25 20:53:36,242 : Iter 350, training loss: 9.929246
2017-12-25 20:53:37,486 : Epoch 1, training loss: 9.923646
2017-12-25 20:53:39,032 : Epoch 1, validation loss: 8.850170
2017-12-25 20:53:39,155 : Training checkpoint has been saved.
2017-12-25 20:53:39,155 : Epoch 2 started:
2017-12-25 20:53:39,430 : Iter 0, training loss: 7.963157
2017-12-25 20:53:41,967 : Iter 10, training loss: 8.298901
2017-12-25 20:53:44,549 : Iter 20, training loss: 9.173698
2017-12-25 20:53:47,208 : Iter 30, training loss: 8.627275
2017-12-25 20:53:49,715 : Iter 40, training loss: 8.256786
2017-12-25 20:53:52,299 : Iter 50, training loss: 8.139090
2017-12-25 20:53:54,867 : Iter 60, training loss: 7.937227
2017-12-25 20:53:57,335 : Iter 70, training loss: 8.022364
2017-12-25 20:53:59,771 : Iter 80, training loss: 7.987826
2017-12-25 20:54:02,254 : Iter 90, training loss: 8.142678
2017-12-25 20:54:04,807 : Iter 100, training loss: 7.917452
2017-12-25 20:54:07,190 : Iter 110, training loss: 7.862067
2017-12-25 20:54:09,656 : Iter 120, training loss: 8.005919
2017-12-25 20:54:12,206 : Iter 130, training loss: 7.972980
2017-12-25 20:54:14,860 : Iter 140, training loss: 7.913603
2017-12-25 20:54:17,476 : Iter 150, training loss: 7.928300
2017-12-25 20:54:20,065 : Iter 160, training loss: 7.928986
2017-12-25 20:54:22,473 : Iter 170, training loss: 7.872837
2017-12-25 20:54:24,994 : Iter 180, training loss: 7.872721
2017-12-25 20:54:27,564 : Iter 190, training loss: 7.808840
2017-12-25 20:54:30,325 : Iter 200, training loss: 7.760877
2017-12-25 20:54:32,766 : Iter 210, training loss: 7.704139
2017-12-25 20:54:35,166 : Iter 220, training loss: 7.701701
2017-12-25 20:54:37,573 : Iter 230, training loss: 7.774442
2017-12-25 20:54:40,084 : Iter 240, training loss: 7.739810
2017-12-25 20:54:42,549 : Iter 250, training loss: 7.764745
2017-12-25 20:54:45,145 : Iter 260, training loss: 7.764598
2017-12-25 20:54:47,507 : Iter 270, training loss: 7.712353
2017-12-25 20:54:50,171 : Iter 280, training loss: 7.654866
2017-12-25 20:54:52,584 : Iter 290, training loss: 7.618420
2017-12-25 20:54:55,262 : Iter 300, training loss: 7.614295
2017-12-25 20:54:57,851 : Iter 310, training loss: 7.611943
2017-12-25 20:55:00,257 : Iter 320, training loss: 7.603820
2017-12-25 20:55:02,912 : Iter 330, training loss: 7.575900
2017-12-25 20:55:05,610 : Iter 340, training loss: 7.575451
2017-12-25 20:55:08,033 : Iter 350, training loss: 7.536574
2017-12-25 20:55:09,221 : Epoch 2, training loss: 7.534568
2017-12-25 20:55:10,887 : Epoch 2, validation loss: 7.935267
2017-12-25 20:55:10,995 : Training checkpoint has been saved.
2017-12-25 20:55:10,995 : Epoch 3 started:
2017-12-25 20:55:11,284 : Iter 0, training loss: 6.849386
2017-12-25 20:55:13,741 : Iter 10, training loss: 5.977739
2017-12-25 20:55:16,307 : Iter 20, training loss: 6.148785
2017-12-25 20:55:18,863 : Iter 30, training loss: 6.165749
2017-12-25 20:55:21,249 : Iter 40, training loss: 6.220501
2017-12-25 20:55:23,687 : Iter 50, training loss: 6.274163
2017-12-25 20:55:26,293 : Iter 60, training loss: 6.297922
2017-12-25 20:55:28,978 : Iter 70, training loss: 6.370996
2017-12-25 20:55:31,469 : Iter 80, training loss: 6.463304
2017-12-25 20:55:33,817 : Iter 90, training loss: 6.443518
2017-12-25 20:55:36,470 : Iter 100, training loss: 6.581516
2017-12-25 20:55:38,824 : Iter 110, training loss: 6.473686
2017-12-25 20:55:41,247 : Iter 120, training loss: 6.485390
2017-12-25 20:55:43,647 : Iter 130, training loss: 6.413901
2017-12-25 20:55:46,111 : Iter 140, training loss: 6.321791
2017-12-25 20:55:48,841 : Iter 150, training loss: 6.284654
2017-12-25 20:55:51,388 : Iter 160, training loss: 6.295620
2017-12-25 20:55:53,875 : Iter 170, training loss: 6.311084
2017-12-25 20:55:56,311 : Iter 180, training loss: 6.353921
2017-12-25 20:55:58,685 : Iter 190, training loss: 6.373404
2017-12-25 20:56:01,104 : Iter 200, training loss: 6.390820
2017-12-25 20:56:03,651 : Iter 210, training loss: 6.374422
2017-12-25 20:56:06,209 : Iter 220, training loss: 6.351029
2017-12-25 20:56:08,669 : Iter 230, training loss: 6.381271
2017-12-25 20:56:10,984 : Iter 240, training loss: 6.369789
2017-12-25 20:56:13,550 : Iter 250, training loss: 6.364202
2017-12-25 20:56:16,041 : Iter 260, training loss: 6.317157
2017-12-25 20:56:18,459 : Iter 270, training loss: 6.428866
2017-12-25 20:56:20,933 : Iter 280, training loss: 6.404868
2017-12-25 20:56:23,343 : Iter 290, training loss: 6.348532
2017-12-25 20:56:25,797 : Iter 300, training loss: 6.372814
2017-12-25 20:56:28,324 : Iter 310, training loss: 6.367697
2017-12-25 20:56:30,829 : Iter 320, training loss: 6.351712
2017-12-25 20:56:33,456 : Iter 330, training loss: 6.367891
2017-12-25 20:56:35,863 : Iter 340, training loss: 6.366523
2017-12-25 20:56:38,208 : Iter 350, training loss: 6.338237
2017-12-25 20:56:39,429 : Epoch 3, training loss: 6.337244
2017-12-25 20:56:41,001 : Epoch 3, validation loss: 6.339905
2017-12-25 20:56:41,099 : Training checkpoint has been saved.
2017-12-25 20:56:41,099 : Epoch 4 started:
2017-12-25 20:56:41,338 : Iter 0, training loss: 4.688032
2017-12-25 20:56:43,731 : Iter 10, training loss: 5.109584
2017-12-25 20:56:46,130 : Iter 20, training loss: 4.900491
2017-12-25 20:56:48,534 : Iter 30, training loss: 5.005637
2017-12-25 20:56:50,998 : Iter 40, training loss: 5.146321
2017-12-25 20:56:53,631 : Iter 50, training loss: 5.392401
2017-12-25 20:56:56,179 : Iter 60, training loss: 5.412667
2017-12-25 20:56:58,649 : Iter 70, training loss: 5.412743
2017-12-25 20:57:01,070 : Iter 80, training loss: 5.372573
2017-12-25 20:57:03,724 : Iter 90, training loss: 5.387731
2017-12-25 20:57:06,170 : Iter 100, training loss: 5.365675
2017-12-25 20:57:08,850 : Iter 110, training loss: 5.359306
2017-12-25 20:57:11,653 : Iter 120, training loss: 5.363027
2017-12-25 20:57:14,042 : Iter 130, training loss: 5.331149
2017-12-25 20:57:16,686 : Iter 140, training loss: 5.247600
2017-12-25 20:57:19,358 : Iter 150, training loss: 5.213144
2017-12-25 20:57:21,775 : Iter 160, training loss: 5.325440
2017-12-25 20:57:24,191 : Iter 170, training loss: 5.600702
2017-12-25 20:57:26,750 : Iter 180, training loss: 5.562272
2017-12-25 20:57:29,328 : Iter 190, training loss: 5.559623
2017-12-25 20:57:31,857 : Iter 200, training loss: 5.518980
2017-12-25 20:57:34,330 : Iter 210, training loss: 5.501996
2017-12-25 20:57:36,692 : Iter 220, training loss: 5.488486
2017-12-25 20:57:39,102 : Iter 230, training loss: 5.454528
2017-12-25 20:57:41,559 : Iter 240, training loss: 5.442487
2017-12-25 20:57:44,012 : Iter 250, training loss: 5.490528
2017-12-25 20:57:46,520 : Iter 260, training loss: 5.530813
2017-12-25 20:57:49,029 : Iter 270, training loss: 5.516895
2017-12-25 20:57:51,527 : Iter 280, training loss: 5.495852
2017-12-25 20:57:54,107 : Iter 290, training loss: 5.471527
2017-12-25 20:57:56,719 : Iter 300, training loss: 5.467952
2017-12-25 20:57:59,586 : Iter 310, training loss: 5.478038
2017-12-25 20:58:01,981 : Iter 320, training loss: 5.459903
2017-12-25 20:58:04,307 : Iter 330, training loss: 5.431181
2017-12-25 20:58:06,762 : Iter 340, training loss: 5.441412
2017-12-25 20:58:09,206 : Iter 350, training loss: 5.438019
2017-12-25 20:58:10,424 : Epoch 4, training loss: 5.434229
2017-12-25 20:58:11,965 : Epoch 4, validation loss: 6.482425
2017-12-25 20:58:12,075 : Training checkpoint has been saved.
2017-12-25 20:58:12,076 : Epoch 5 started:
2017-12-25 20:58:12,300 : Iter 0, training loss: 3.940181
2017-12-25 20:58:15,008 : Iter 10, training loss: 4.971798
2017-12-25 20:58:17,705 : Iter 20, training loss: 4.340266
2017-12-25 20:58:20,293 : Iter 30, training loss: 4.309312
2017-12-25 20:58:22,663 : Iter 40, training loss: 4.538676
2017-12-25 20:58:25,198 : Iter 50, training loss: 4.483720
2017-12-25 20:58:27,657 : Iter 60, training loss: 4.530400
2017-12-25 20:58:30,188 : Iter 70, training loss: 4.572195
2017-12-25 20:58:32,766 : Iter 80, training loss: 4.633402
2017-12-25 20:58:35,324 : Iter 90, training loss: 4.683748
2017-12-25 20:58:37,772 : Iter 100, training loss: 4.698141
2017-12-25 20:58:40,316 : Iter 110, training loss: 4.667260
2017-12-25 20:58:42,694 : Iter 120, training loss: 4.718004
2017-12-25 20:58:45,235 : Iter 130, training loss: 4.814173
2017-12-25 20:58:47,851 : Iter 140, training loss: 4.785525
2017-12-25 20:58:50,438 : Iter 150, training loss: 4.805325
2017-12-25 20:58:52,789 : Iter 160, training loss: 4.770702
2017-12-25 20:58:55,510 : Iter 170, training loss: 4.759137
2017-12-25 20:58:58,058 : Iter 180, training loss: 4.752048
2017-12-25 20:59:00,537 : Iter 190, training loss: 4.749607
2017-12-25 20:59:02,941 : Iter 200, training loss: 4.759427
2017-12-25 20:59:05,605 : Iter 210, training loss: 4.748421
2017-12-25 20:59:08,090 : Iter 220, training loss: 4.721163
2017-12-25 20:59:10,458 : Iter 230, training loss: 4.754331
2017-12-25 20:59:13,069 : Iter 240, training loss: 4.744617
2017-12-25 20:59:15,550 : Iter 250, training loss: 4.724336
2017-12-25 20:59:17,952 : Iter 260, training loss: 4.701390
2017-12-25 20:59:20,501 : Iter 270, training loss: 4.680098
2017-12-25 20:59:23,021 : Iter 280, training loss: 4.668179
2017-12-25 20:59:25,567 : Iter 290, training loss: 4.691453
2017-12-25 20:59:27,998 : Iter 300, training loss: 4.698059
2017-12-25 20:59:30,559 : Iter 310, training loss: 4.689427
2017-12-25 20:59:32,927 : Iter 320, training loss: 4.670322
2017-12-25 20:59:35,539 : Iter 330, training loss: 4.669553
2017-12-25 20:59:37,984 : Iter 340, training loss: 4.659360
2017-12-25 20:59:40,505 : Iter 350, training loss: 4.683932
2017-12-25 20:59:41,878 : Epoch 5, training loss: 4.689658
2017-12-25 20:59:43,721 : Epoch 5, validation loss: 5.630928
2017-12-25 20:59:43,882 : Training checkpoint has been saved.
2017-12-25 20:59:43,882 : Epoch 6 started:
2017-12-25 20:59:44,137 : Iter 0, training loss: 4.028228
2017-12-25 20:59:46,409 : Iter 10, training loss: 3.553389
2017-12-25 20:59:49,068 : Iter 20, training loss: 3.647261
2017-12-25 20:59:51,577 : Iter 30, training loss: 3.590781
2017-12-25 20:59:54,219 : Iter 40, training loss: 3.548713
2017-12-25 20:59:56,763 : Iter 50, training loss: 3.583558
2017-12-25 20:59:59,370 : Iter 60, training loss: 3.711994
2017-12-25 21:00:01,765 : Iter 70, training loss: 4.268487
2017-12-25 21:00:04,410 : Iter 80, training loss: 4.269364
2017-12-25 21:00:06,750 : Iter 90, training loss: 4.244034
2017-12-25 21:00:09,079 : Iter 100, training loss: 4.199297
2017-12-25 21:00:11,715 : Iter 110, training loss: 4.161065
2017-12-25 21:00:14,396 : Iter 120, training loss: 4.129197
2017-12-25 21:00:16,986 : Iter 130, training loss: 4.160959
2017-12-25 21:00:19,543 : Iter 140, training loss: 4.195422
2017-12-25 21:00:22,109 : Iter 150, training loss: 4.205971
2017-12-25 21:00:24,690 : Iter 160, training loss: 4.203393
2017-12-25 21:00:27,303 : Iter 170, training loss: 4.220092
2017-12-25 21:00:30,092 : Iter 180, training loss: 4.288383
2017-12-25 21:00:32,680 : Iter 190, training loss: 4.300791
2017-12-25 21:00:35,519 : Iter 200, training loss: 4.270132
2017-12-25 21:00:38,006 : Iter 210, training loss: 4.267851
2017-12-25 21:00:40,669 : Iter 220, training loss: 4.248843
2017-12-25 21:00:43,046 : Iter 230, training loss: 4.243976
2017-12-25 21:00:45,816 : Iter 240, training loss: 4.234676
2017-12-25 21:00:48,495 : Iter 250, training loss: 4.228082
2017-12-25 21:00:50,984 : Iter 260, training loss: 4.204364
2017-12-25 21:00:53,490 : Iter 270, training loss: 4.196405
2017-12-25 21:00:56,388 : Iter 280, training loss: 4.184347
2017-12-25 21:00:58,931 : Iter 290, training loss: 4.189555
2017-12-25 21:01:01,557 : Iter 300, training loss: 4.190902
2017-12-25 21:01:03,991 : Iter 310, training loss: 4.212399
2017-12-25 21:01:06,516 : Iter 320, training loss: 4.206841
2017-12-25 21:01:09,022 : Iter 330, training loss: 4.202935
2017-12-25 21:01:11,580 : Iter 340, training loss: 4.210504
2017-12-25 21:01:14,281 : Iter 350, training loss: 4.208466
2017-12-25 21:01:15,480 : Epoch 6, training loss: 4.200623
2017-12-25 21:01:17,056 : Epoch 6, validation loss: 5.470647
2017-12-25 21:01:17,160 : Training checkpoint has been saved.
2017-12-25 21:01:17,160 : Epoch 7 started:
2017-12-25 21:01:17,409 : Iter 0, training loss: 3.697375
2017-12-25 21:01:19,838 : Iter 10, training loss: 3.510684
2017-12-25 21:01:22,301 : Iter 20, training loss: 3.771432
2017-12-25 21:01:24,824 : Iter 30, training loss: 3.833043
2017-12-25 21:01:27,276 : Iter 40, training loss: 3.809896
2017-12-25 21:01:30,065 : Iter 50, training loss: 3.871860
2017-12-25 21:01:32,715 : Iter 60, training loss: 3.780327
2017-12-25 21:01:35,267 : Iter 70, training loss: 3.767351
2017-12-25 21:01:37,678 : Iter 80, training loss: 3.754249
2017-12-25 21:01:40,015 : Iter 90, training loss: 3.837833
2017-12-25 21:01:42,582 : Iter 100, training loss: 3.798340
2017-12-25 21:01:45,055 : Iter 110, training loss: 3.777584
2017-12-25 21:01:47,450 : Iter 120, training loss: 3.745631
2017-12-25 21:01:49,993 : Iter 130, training loss: 3.751445
2017-12-25 21:01:52,640 : Iter 140, training loss: 3.738225
2017-12-25 21:01:55,227 : Iter 150, training loss: 3.735924
2017-12-25 21:01:57,760 : Iter 160, training loss: 3.788749
2017-12-25 21:02:00,325 : Iter 170, training loss: 3.782475
2017-12-25 21:02:02,989 : Iter 180, training loss: 3.778579
2017-12-25 21:02:05,376 : Iter 190, training loss: 3.756458
2017-12-25 21:02:08,086 : Iter 200, training loss: 3.726169
2017-12-25 21:02:10,782 : Iter 210, training loss: 3.702884
2017-12-25 21:02:13,241 : Iter 220, training loss: 3.710267
2017-12-25 21:02:15,659 : Iter 230, training loss: 3.714308
2017-12-25 21:02:18,447 : Iter 240, training loss: 3.732510
2017-12-25 21:02:20,934 : Iter 250, training loss: 3.732224
2017-12-25 21:02:23,507 : Iter 260, training loss: 3.781894
2017-12-25 21:02:25,983 : Iter 270, training loss: 3.763969
2017-12-25 21:02:28,539 : Iter 280, training loss: 3.758579
2017-12-25 21:02:30,929 : Iter 290, training loss: 3.764385
2017-12-25 21:02:33,511 : Iter 300, training loss: 3.755908
2017-12-25 21:02:35,922 : Iter 310, training loss: 3.782826
2017-12-25 21:02:38,295 : Iter 320, training loss: 3.786024
2017-12-25 21:02:40,737 : Iter 330, training loss: 3.809627
2017-12-25 21:02:43,142 : Iter 340, training loss: 3.823297
2017-12-25 21:02:45,721 : Iter 350, training loss: 3.817045
2017-12-25 21:02:46,908 : Epoch 7, training loss: 3.814523
2017-12-25 21:02:48,468 : Epoch 7, validation loss: 5.404499
2017-12-25 21:02:48,563 : Training checkpoint has been saved.
2017-12-25 21:02:48,563 : Epoch 8 started:
2017-12-25 21:02:48,794 : Iter 0, training loss: 2.299101
2017-12-25 21:02:51,182 : Iter 10, training loss: 2.987315
2017-12-25 21:02:53,562 : Iter 20, training loss: 2.971024
2017-12-25 21:02:56,060 : Iter 30, training loss: 2.896214
2017-12-25 21:02:58,785 : Iter 40, training loss: 3.023734
2017-12-25 21:03:01,457 : Iter 50, training loss: 3.044024
2017-12-25 21:03:03,886 : Iter 60, training loss: 3.073148
2017-12-25 21:03:06,303 : Iter 70, training loss: 3.093142
2017-12-25 21:03:08,852 : Iter 80, training loss: 3.247854
2017-12-25 21:03:11,294 : Iter 90, training loss: 3.231097
2017-12-25 21:03:13,802 : Iter 100, training loss: 3.202262
2017-12-25 21:03:16,201 : Iter 110, training loss: 3.203052
2017-12-25 21:03:18,855 : Iter 120, training loss: 3.219525
2017-12-25 21:03:21,365 : Iter 130, training loss: 3.290232
2017-12-25 21:03:23,960 : Iter 140, training loss: 3.311204
2017-12-25 21:03:26,378 : Iter 150, training loss: 3.309338
2017-12-25 21:03:28,790 : Iter 160, training loss: 3.295571
2017-12-25 21:03:31,437 : Iter 170, training loss: 3.275078
2017-12-25 21:03:33,871 : Iter 180, training loss: 3.272965
2017-12-25 21:03:36,268 : Iter 190, training loss: 3.275450
2017-12-25 21:03:38,800 : Iter 200, training loss: 3.278645
2017-12-25 21:03:41,179 : Iter 210, training loss: 3.269023
2017-12-25 21:03:43,762 : Iter 220, training loss: 3.284166
2017-12-25 21:03:46,155 : Iter 230, training loss: 3.267832
2017-12-25 21:03:48,770 : Iter 240, training loss: 3.286313
2017-12-25 21:03:51,235 : Iter 250, training loss: 3.277472
2017-12-25 21:03:53,814 : Iter 260, training loss: 3.279698
2017-12-25 21:03:56,328 : Iter 270, training loss: 3.286378
2017-12-25 21:03:58,949 : Iter 280, training loss: 3.292783
2017-12-25 21:04:01,440 : Iter 290, training loss: 3.303871
2017-12-25 21:04:03,890 : Iter 300, training loss: 3.309797
2017-12-25 21:04:06,445 : Iter 310, training loss: 3.311314
2017-12-25 21:04:08,979 : Iter 320, training loss: 3.345830
2017-12-25 21:04:11,434 : Iter 330, training loss: 3.337670
2017-12-25 21:04:13,769 : Iter 340, training loss: 3.346833
2017-12-25 21:04:16,272 : Iter 350, training loss: 3.354823
2017-12-25 21:04:17,733 : Epoch 8, training loss: 3.353883
2017-12-25 21:04:19,463 : Epoch 8, validation loss: 5.383278
2017-12-25 21:04:19,569 : Training checkpoint has been saved.
2017-12-25 21:04:19,569 : Epoch 9 started:
2017-12-25 21:04:19,795 : Iter 0, training loss: 2.254768
2017-12-25 21:04:22,336 : Iter 10, training loss: 2.985203
2017-12-25 21:04:24,617 : Iter 20, training loss: 3.109142
2017-12-25 21:04:27,037 : Iter 30, training loss: 3.166351
2017-12-25 21:04:29,592 : Iter 40, training loss: 3.205867
2017-12-25 21:04:32,163 : Iter 50, training loss: 3.103371
2017-12-25 21:04:34,795 : Iter 60, training loss: 3.085758
2017-12-25 21:04:37,319 : Iter 70, training loss: 3.030716
2017-12-25 21:04:39,738 : Iter 80, training loss: 2.983712
2017-12-25 21:04:42,186 : Iter 90, training loss: 2.939498
2017-12-25 21:04:44,499 : Iter 100, training loss: 2.890396
2017-12-25 21:04:46,966 : Iter 110, training loss: 2.891980
2017-12-25 21:04:49,328 : Iter 120, training loss: 2.900292
2017-12-25 21:04:51,819 : Iter 130, training loss: 2.885817
2017-12-25 21:04:54,297 : Iter 140, training loss: 2.946673
2017-12-25 21:04:56,920 : Iter 150, training loss: 2.956798
2017-12-25 21:04:59,406 : Iter 160, training loss: 2.955208
2017-12-25 21:05:01,953 : Iter 170, training loss: 2.964859
2017-12-25 21:05:04,609 : Iter 180, training loss: 2.986878
2017-12-25 21:05:07,134 : Iter 190, training loss: 2.979762
2017-12-25 21:05:09,753 : Iter 200, training loss: 2.974440
2017-12-25 21:05:12,050 : Iter 210, training loss: 2.958655
2017-12-25 21:05:14,551 : Iter 220, training loss: 2.951003
2017-12-25 21:05:17,224 : Iter 230, training loss: 2.957830
2017-12-25 21:05:19,585 : Iter 240, training loss: 2.975702
2017-12-25 21:05:21,937 : Iter 250, training loss: 2.981870
2017-12-25 21:05:24,391 : Iter 260, training loss: 2.991420
2017-12-25 21:05:26,782 : Iter 270, training loss: 2.989804
2017-12-25 21:05:29,373 : Iter 280, training loss: 2.991810
2017-12-25 21:05:31,795 : Iter 290, training loss: 3.006888
2017-12-25 21:05:34,486 : Iter 300, training loss: 2.996424
2017-12-25 21:05:36,932 : Iter 310, training loss: 2.991711
2017-12-25 21:05:39,470 : Iter 320, training loss: 2.990556
2017-12-25 21:05:41,795 : Iter 330, training loss: 2.992474
2017-12-25 21:05:44,327 : Iter 340, training loss: 2.999435
2017-12-25 21:05:46,909 : Iter 350, training loss: 2.996642
2017-12-25 21:05:48,177 : Epoch 9, training loss: 2.993394
2017-12-25 21:05:49,801 : Epoch 9, validation loss: 5.322185
2017-12-25 21:05:49,937 : Training checkpoint has been saved.
2017-12-25 21:05:49,955 : Training process has been finished.
