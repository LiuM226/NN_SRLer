/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
2017-12-25 21:48:49,926 : Finished building computation graph for training.
2017-12-25 21:48:52,522 : Read in training data successfully.
2017-12-25 21:48:52.548060: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2017-12-25 21:48:53.373907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.873
pciBusID: 0000:03:00.0
totalMemory: 7.92GiB freeMemory: 7.80GiB
2017-12-25 21:48:53.373940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)
2017-12-25 21:48:54,127 : Epoch 0 started:
2017-12-25 21:48:54,682 : Iter 0, training loss: 691.230469
2017-12-25 21:48:57,235 : Iter 10, training loss: 867.309006
2017-12-25 21:48:59,883 : Iter 20, training loss: 881.758103
2017-12-25 21:49:02,462 : Iter 30, training loss: 856.621355
2017-12-25 21:49:05,001 : Iter 40, training loss: 789.425937
2017-12-25 21:49:07,402 : Iter 50, training loss: 724.771078
2017-12-25 21:49:09,796 : Iter 60, training loss: 664.980728
2017-12-25 21:49:12,203 : Iter 70, training loss: 598.154978
2017-12-25 21:49:14,715 : Iter 80, training loss: 549.613556
2017-12-25 21:49:17,255 : Iter 90, training loss: 526.959913
2017-12-25 21:49:19,926 : Iter 100, training loss: 492.184726
2017-12-25 21:49:22,266 : Iter 110, training loss: 459.753986
2017-12-25 21:49:24,846 : Iter 120, training loss: 434.196408
2017-12-25 21:49:27,473 : Iter 130, training loss: 417.209818
2017-12-25 21:49:30,076 : Iter 140, training loss: 398.158764
2017-12-25 21:49:32,614 : Iter 150, training loss: 380.968764
2017-12-25 21:49:35,325 : Iter 160, training loss: 362.289495
2017-12-25 21:49:37,788 : Iter 170, training loss: 347.182282
2017-12-25 21:49:40,300 : Iter 180, training loss: 332.836670
2017-12-25 21:49:42,748 : Iter 190, training loss: 321.127792
2017-12-25 21:49:45,236 : Iter 200, training loss: 309.442341
2017-12-25 21:49:47,632 : Iter 210, training loss: 299.073042
2017-12-25 21:49:50,118 : Iter 220, training loss: 288.503824
2017-12-25 21:49:52,666 : Iter 230, training loss: 279.077400
2017-12-25 21:49:55,177 : Iter 240, training loss: 270.454425
2017-12-25 21:49:57,682 : Iter 250, training loss: 262.597466
2017-12-25 21:50:00,226 : Iter 260, training loss: 255.256577
2017-12-25 21:50:02,858 : Iter 270, training loss: 249.061794
2017-12-25 21:50:05,344 : Iter 280, training loss: 242.487595
2017-12-25 21:50:07,774 : Iter 290, training loss: 236.159430
2017-12-25 21:50:10,202 : Iter 300, training loss: 230.941901
2017-12-25 21:50:12,566 : Iter 310, training loss: 225.075073
2017-12-25 21:50:15,028 : Iter 320, training loss: 220.213024
2017-12-25 21:50:17,585 : Iter 330, training loss: 215.705332
2017-12-25 21:50:20,079 : Iter 340, training loss: 211.143110
2017-12-25 21:50:22,619 : Iter 350, training loss: 206.679125
2017-12-25 21:50:23,829 : Epoch 0, training loss: 204.749534
2017-12-25 21:50:25,592 : Epoch 0, validation loss: 89.978243
2017-12-25 21:50:25,742 : Training checkpoint has been saved.
2017-12-25 21:50:25,742 : Epoch 1 started:
2017-12-25 21:50:26,040 : Iter 0, training loss: 85.764336
2017-12-25 21:50:28,472 : Iter 10, training loss: 50.161962
2017-12-25 21:50:31,014 : Iter 20, training loss: 46.508033
2017-12-25 21:50:33,867 : Iter 30, training loss: 52.043023
2017-12-25 21:50:36,488 : Iter 40, training loss: 52.250513
2017-12-25 21:50:39,162 : Iter 50, training loss: 50.794948
2017-12-25 21:50:41,627 : Iter 60, training loss: 48.073903
2017-12-25 21:50:44,227 : Iter 70, training loss: 49.581850
2017-12-25 21:50:46,639 : Iter 80, training loss: 49.241403
2017-12-25 21:50:49,030 : Iter 90, training loss: 47.930020
2017-12-25 21:50:51,529 : Iter 100, training loss: 47.437786
2017-12-25 21:50:54,064 : Iter 110, training loss: 48.002931
2017-12-25 21:50:56,648 : Iter 120, training loss: 47.657936
2017-12-25 21:50:59,113 : Iter 130, training loss: 47.907497
2017-12-25 21:51:01,747 : Iter 140, training loss: 47.920457
2017-12-25 21:51:04,252 : Iter 150, training loss: 48.559165
2017-12-25 21:51:07,031 : Iter 160, training loss: 49.524997
2017-12-25 21:51:09,405 : Iter 170, training loss: 49.142041
2017-12-25 21:51:11,975 : Iter 180, training loss: 48.697279
2017-12-25 21:51:14,549 : Iter 190, training loss: 48.180642
2017-12-25 21:51:17,063 : Iter 200, training loss: 49.297579
2017-12-25 21:51:19,630 : Iter 210, training loss: 48.993130
2017-12-25 21:51:22,309 : Iter 220, training loss: 49.094385
2017-12-25 21:51:24,889 : Iter 230, training loss: 48.535878
2017-12-25 21:51:27,311 : Iter 240, training loss: 48.504873
2017-12-25 21:51:29,736 : Iter 250, training loss: 48.039572
2017-12-25 21:51:32,287 : Iter 260, training loss: 47.997650
2017-12-25 21:51:34,621 : Iter 270, training loss: 47.936860
2017-12-25 21:51:37,256 : Iter 280, training loss: 47.483047
2017-12-25 21:51:39,911 : Iter 290, training loss: 46.675255
2017-12-25 21:51:42,270 : Iter 300, training loss: 46.196865
2017-12-25 21:51:44,667 : Iter 310, training loss: 45.600605
2017-12-25 21:51:47,068 : Iter 320, training loss: 45.203538
2017-12-25 21:51:49,524 : Iter 330, training loss: 44.716399
2017-12-25 21:51:52,208 : Iter 340, training loss: 44.764287
2017-12-25 21:51:54,782 : Iter 350, training loss: 44.623236
2017-12-25 21:51:55,946 : Epoch 1, training loss: 44.414550
2017-12-25 21:51:57,527 : Epoch 1, validation loss: 27.642554
2017-12-25 21:51:57,681 : Training checkpoint has been saved.
2017-12-25 21:51:57,681 : Epoch 2 started:
2017-12-25 21:51:58,030 : Iter 0, training loss: 20.717256
2017-12-25 21:52:00,438 : Iter 10, training loss: 26.905607
2017-12-25 21:52:02,992 : Iter 20, training loss: 28.793202
2017-12-25 21:52:05,346 : Iter 30, training loss: 30.142028
2017-12-25 21:52:07,830 : Iter 40, training loss: 30.872648
2017-12-25 21:52:10,559 : Iter 50, training loss: 29.992896
2017-12-25 21:52:13,108 : Iter 60, training loss: 28.902407
2017-12-25 21:52:15,622 : Iter 70, training loss: 28.968081
2017-12-25 21:52:18,047 : Iter 80, training loss: 28.431465
2017-12-25 21:52:20,580 : Iter 90, training loss: 29.016510
2017-12-25 21:52:23,125 : Iter 100, training loss: 28.262073
2017-12-25 21:52:25,839 : Iter 110, training loss: 27.689019
2017-12-25 21:52:28,184 : Iter 120, training loss: 28.864621
2017-12-25 21:52:30,602 : Iter 130, training loss: 29.051636
2017-12-25 21:52:32,986 : Iter 140, training loss: 28.452410
2017-12-25 21:52:35,574 : Iter 150, training loss: 28.138956
2017-12-25 21:52:38,124 : Iter 160, training loss: 28.778206
2017-12-25 21:52:40,566 : Iter 170, training loss: 28.592763
2017-12-25 21:52:43,230 : Iter 180, training loss: 28.552415
2017-12-25 21:52:45,778 : Iter 190, training loss: 28.156300
2017-12-25 21:52:48,181 : Iter 200, training loss: 28.444758
2017-12-25 21:52:50,545 : Iter 210, training loss: 28.093764
2017-12-25 21:52:53,106 : Iter 220, training loss: 28.937782
2017-12-25 21:52:55,735 : Iter 230, training loss: 28.763882
2017-12-25 21:52:58,176 : Iter 240, training loss: 28.951798
2017-12-25 21:53:00,541 : Iter 250, training loss: 28.994653
2017-12-25 21:53:03,038 : Iter 260, training loss: 28.969573
2017-12-25 21:53:05,775 : Iter 270, training loss: 29.580049
2017-12-25 21:53:08,126 : Iter 280, training loss: 29.731116
2017-12-25 21:53:10,726 : Iter 290, training loss: 29.736471
2017-12-25 21:53:13,236 : Iter 300, training loss: 29.493486
2017-12-25 21:53:15,809 : Iter 310, training loss: 29.438439
2017-12-25 21:53:18,337 : Iter 320, training loss: 29.282620
2017-12-25 21:53:20,877 : Iter 330, training loss: 29.339219
2017-12-25 21:53:23,376 : Iter 340, training loss: 29.125662
2017-12-25 21:53:25,990 : Iter 350, training loss: 28.837672
2017-12-25 21:53:27,157 : Epoch 2, training loss: 28.967276
2017-12-25 21:53:28,877 : Epoch 2, validation loss: 21.925234
2017-12-25 21:53:29,000 : Training checkpoint has been saved.
2017-12-25 21:53:29,000 : Epoch 3 started:
2017-12-25 21:53:29,235 : Iter 0, training loss: 15.772693
2017-12-25 21:53:31,832 : Iter 10, training loss: 15.081326
2017-12-25 21:53:34,331 : Iter 20, training loss: 18.964220
2017-12-25 21:53:36,859 : Iter 30, training loss: 18.775909
2017-12-25 21:53:39,411 : Iter 40, training loss: 19.650203
2017-12-25 21:53:41,834 : Iter 50, training loss: 20.206585
2017-12-25 21:53:44,267 : Iter 60, training loss: 21.122427
2017-12-25 21:53:46,871 : Iter 70, training loss: 22.013820
2017-12-25 21:53:49,387 : Iter 80, training loss: 22.448919
2017-12-25 21:53:51,940 : Iter 90, training loss: 21.958627
2017-12-25 21:53:54,446 : Iter 100, training loss: 21.722011
2017-12-25 21:53:56,902 : Iter 110, training loss: 22.328508
2017-12-25 21:53:59,518 : Iter 120, training loss: 22.360534
2017-12-25 21:54:02,062 : Iter 130, training loss: 21.947534
2017-12-25 21:54:04,633 : Iter 140, training loss: 21.266952
2017-12-25 21:54:07,254 : Iter 150, training loss: 21.267011
2017-12-25 21:54:09,578 : Iter 160, training loss: 21.788219
2017-12-25 21:54:12,130 : Iter 170, training loss: 21.775384
2017-12-25 21:54:14,567 : Iter 180, training loss: 21.874538
2017-12-25 21:54:17,100 : Iter 190, training loss: 21.906329
2017-12-25 21:54:19,623 : Iter 200, training loss: 22.554529
2017-12-25 21:54:22,260 : Iter 210, training loss: 22.369225
2017-12-25 21:54:24,655 : Iter 220, training loss: 22.329563
2017-12-25 21:54:27,172 : Iter 230, training loss: 22.467321
2017-12-25 21:54:29,547 : Iter 240, training loss: 22.444107
2017-12-25 21:54:32,164 : Iter 250, training loss: 22.267427
2017-12-25 21:54:34,849 : Iter 260, training loss: 22.043383
2017-12-25 21:54:37,615 : Iter 270, training loss: 21.844093
2017-12-25 21:54:40,024 : Iter 280, training loss: 21.656232
2017-12-25 21:54:42,427 : Iter 290, training loss: 21.663223
2017-12-25 21:54:44,936 : Iter 300, training loss: 21.557346
2017-12-25 21:54:47,324 : Iter 310, training loss: 21.545920
2017-12-25 21:54:49,778 : Iter 320, training loss: 21.501068
2017-12-25 21:54:52,223 : Iter 330, training loss: 21.493570
2017-12-25 21:54:54,753 : Iter 340, training loss: 21.303884
2017-12-25 21:54:57,200 : Iter 350, training loss: 21.251613
2017-12-25 21:54:58,355 : Epoch 3, training loss: 21.299436
2017-12-25 21:55:00,051 : Epoch 3, validation loss: 18.269922
2017-12-25 21:55:00,152 : Training checkpoint has been saved.
2017-12-25 21:55:00,152 : Epoch 4 started:
2017-12-25 21:55:00,372 : Iter 0, training loss: 10.926825
2017-12-25 21:55:03,157 : Iter 10, training loss: 12.009365
2017-12-25 21:55:05,540 : Iter 20, training loss: 13.864755
2017-12-25 21:55:08,100 : Iter 30, training loss: 13.989219
2017-12-25 21:55:10,754 : Iter 40, training loss: 13.660601
2017-12-25 21:55:13,431 : Iter 50, training loss: 15.269315
2017-12-25 21:55:16,011 : Iter 60, training loss: 15.373755
2017-12-25 21:55:18,649 : Iter 70, training loss: 15.180835
2017-12-25 21:55:21,296 : Iter 80, training loss: 14.996987
2017-12-25 21:55:23,802 : Iter 90, training loss: 15.788356
2017-12-25 21:55:26,254 : Iter 100, training loss: 15.794137
2017-12-25 21:55:28,839 : Iter 110, training loss: 16.058070
2017-12-25 21:55:31,284 : Iter 120, training loss: 15.571511
2017-12-25 21:55:33,801 : Iter 130, training loss: 15.882020
2017-12-25 21:55:36,341 : Iter 140, training loss: 15.601717
2017-12-25 21:55:38,747 : Iter 150, training loss: 15.732149
2017-12-25 21:55:41,344 : Iter 160, training loss: 15.913737
2017-12-25 21:55:43,979 : Iter 170, training loss: 16.612970
2017-12-25 21:55:46,424 : Iter 180, training loss: 16.330746
2017-12-25 21:55:48,845 : Iter 190, training loss: 16.041412
2017-12-25 21:55:51,322 : Iter 200, training loss: 16.294871
2017-12-25 21:55:53,840 : Iter 210, training loss: 16.086346
2017-12-25 21:55:56,198 : Iter 220, training loss: 16.067219
2017-12-25 21:55:58,806 : Iter 230, training loss: 15.994079
2017-12-25 21:56:01,393 : Iter 240, training loss: 16.254741
2017-12-25 21:56:04,067 : Iter 250, training loss: 16.417719
2017-12-25 21:56:06,442 : Iter 260, training loss: 16.642377
2017-12-25 21:56:08,780 : Iter 270, training loss: 16.413152
2017-12-25 21:56:11,279 : Iter 280, training loss: 16.773905
2017-12-25 21:56:13,816 : Iter 290, training loss: 16.687661
2017-12-25 21:56:16,272 : Iter 300, training loss: 16.479939
2017-12-25 21:56:18,725 : Iter 310, training loss: 16.761935
2017-12-25 21:56:21,306 : Iter 320, training loss: 16.974570
2017-12-25 21:56:23,771 : Iter 330, training loss: 16.870522
2017-12-25 21:56:26,420 : Iter 340, training loss: 16.859326
2017-12-25 21:56:28,958 : Iter 350, training loss: 16.789560
2017-12-25 21:56:30,158 : Epoch 4, training loss: 16.937614
2017-12-25 21:56:31,711 : Epoch 4, validation loss: 19.966710
2017-12-25 21:56:31,809 : Training checkpoint has been saved.
2017-12-25 21:56:31,810 : Epoch 5 started:
2017-12-25 21:56:32,059 : Iter 0, training loss: 9.991239
2017-12-25 21:56:34,626 : Iter 10, training loss: 13.610701
2017-12-25 21:56:37,097 : Iter 20, training loss: 11.709698
2017-12-25 21:56:39,397 : Iter 30, training loss: 11.739109
2017-12-25 21:56:41,802 : Iter 40, training loss: 13.740696
2017-12-25 21:56:44,375 : Iter 50, training loss: 12.892823
2017-12-25 21:56:46,837 : Iter 60, training loss: 13.777853
2017-12-25 21:56:49,288 : Iter 70, training loss: 13.326712
2017-12-25 21:56:51,826 : Iter 80, training loss: 14.404168
2017-12-25 21:56:54,350 : Iter 90, training loss: 14.348780
2017-12-25 21:56:56,882 : Iter 100, training loss: 14.687375
2017-12-25 21:56:59,436 : Iter 110, training loss: 14.424536
2017-12-25 21:57:01,830 : Iter 120, training loss: 14.223166
2017-12-25 21:57:04,291 : Iter 130, training loss: 14.722807
2017-12-25 21:57:06,878 : Iter 140, training loss: 14.548070
2017-12-25 21:57:09,410 : Iter 150, training loss: 14.466196
2017-12-25 21:57:11,961 : Iter 160, training loss: 14.242202
2017-12-25 21:57:14,645 : Iter 170, training loss: 14.077651
2017-12-25 21:57:17,088 : Iter 180, training loss: 14.034480
2017-12-25 21:57:19,500 : Iter 190, training loss: 13.933157
2017-12-25 21:57:22,198 : Iter 200, training loss: 14.099849
2017-12-25 21:57:24,642 : Iter 210, training loss: 13.862143
2017-12-25 21:57:26,916 : Iter 220, training loss: 14.056343
2017-12-25 21:57:29,599 : Iter 230, training loss: 13.962666
2017-12-25 21:57:32,117 : Iter 240, training loss: 14.096936
2017-12-25 21:57:34,675 : Iter 250, training loss: 13.888839
2017-12-25 21:57:37,246 : Iter 260, training loss: 13.662978
2017-12-25 21:57:39,597 : Iter 270, training loss: 13.765553
2017-12-25 21:57:42,006 : Iter 280, training loss: 13.611985
2017-12-25 21:57:44,474 : Iter 290, training loss: 13.581852
2017-12-25 21:57:46,914 : Iter 300, training loss: 13.574481
2017-12-25 21:57:49,244 : Iter 310, training loss: 13.426165
2017-12-25 21:57:51,680 : Iter 320, training loss: 13.842100
2017-12-25 21:57:54,096 : Iter 330, training loss: 13.729035
2017-12-25 21:57:56,453 : Iter 340, training loss: 13.721953
2017-12-25 21:57:59,038 : Iter 350, training loss: 13.807116
2017-12-25 21:58:00,267 : Epoch 5, training loss: 13.766390
2017-12-25 21:58:01,802 : Epoch 5, validation loss: 13.977047
2017-12-25 21:58:01,901 : Training checkpoint has been saved.
2017-12-25 21:58:01,901 : Epoch 6 started:
2017-12-25 21:58:02,156 : Iter 0, training loss: 7.752838
2017-12-25 21:58:04,522 : Iter 10, training loss: 7.576413
2017-12-25 21:58:06,965 : Iter 20, training loss: 7.835982
2017-12-25 21:58:09,492 : Iter 30, training loss: 7.995377
2017-12-25 21:58:12,033 : Iter 40, training loss: 8.781719
2017-12-25 21:58:14,537 : Iter 50, training loss: 8.944430
2017-12-25 21:58:17,104 : Iter 60, training loss: 9.014376
2017-12-25 21:58:19,667 : Iter 70, training loss: 8.683015
2017-12-25 21:58:22,175 : Iter 80, training loss: 8.852541
2017-12-25 21:58:24,654 : Iter 90, training loss: 9.071475
2017-12-25 21:58:27,182 : Iter 100, training loss: 8.825629
2017-12-25 21:58:29,802 : Iter 110, training loss: 9.297820
2017-12-25 21:58:32,290 : Iter 120, training loss: 9.110406
2017-12-25 21:58:34,566 : Iter 130, training loss: 9.312306
2017-12-25 21:58:37,177 : Iter 140, training loss: 9.539893
2017-12-25 21:58:39,782 : Iter 150, training loss: 9.628474
2017-12-25 21:58:42,381 : Iter 160, training loss: 9.593752
2017-12-25 21:58:44,763 : Iter 170, training loss: 10.071078
2017-12-25 21:58:47,313 : Iter 180, training loss: 9.973021
2017-12-25 21:58:49,882 : Iter 190, training loss: 9.897421
2017-12-25 21:58:52,401 : Iter 200, training loss: 9.907134
2017-12-25 21:58:54,845 : Iter 210, training loss: 10.021341
2017-12-25 21:58:57,318 : Iter 220, training loss: 9.943509
2017-12-25 21:58:59,824 : Iter 230, training loss: 9.979539
2017-12-25 21:59:02,407 : Iter 240, training loss: 9.874807
2017-12-25 21:59:04,892 : Iter 250, training loss: 9.893805
2017-12-25 21:59:07,405 : Iter 260, training loss: 9.832798
2017-12-25 21:59:09,777 : Iter 270, training loss: 9.826777
2017-12-25 21:59:12,278 : Iter 280, training loss: 9.725907
2017-12-25 21:59:14,721 : Iter 290, training loss: 9.757463
2017-12-25 21:59:17,201 : Iter 300, training loss: 9.762841
2017-12-25 21:59:19,717 : Iter 310, training loss: 9.724225
2017-12-25 21:59:22,170 : Iter 320, training loss: 9.783500
2017-12-25 21:59:24,746 : Iter 330, training loss: 9.755244
2017-12-25 21:59:27,369 : Iter 340, training loss: 9.951464
2017-12-25 21:59:29,671 : Iter 350, training loss: 9.975348
2017-12-25 21:59:30,866 : Epoch 6, training loss: 10.093509
2017-12-25 21:59:32,624 : Epoch 6, validation loss: 29.610331
2017-12-25 21:59:32,746 : Training checkpoint has been saved.
2017-12-25 21:59:32,746 : Epoch 7 started:
2017-12-25 21:59:32,959 : Iter 0, training loss: 20.322856
2017-12-25 21:59:35,260 : Iter 10, training loss: 8.124684
2017-12-25 21:59:37,858 : Iter 20, training loss: 9.316300
2017-12-25 21:59:40,520 : Iter 30, training loss: 9.019029
2017-12-25 21:59:43,090 : Iter 40, training loss: 8.783117
2017-12-25 21:59:45,785 : Iter 50, training loss: 8.292561
2017-12-25 21:59:48,296 : Iter 60, training loss: 8.187671
2017-12-25 21:59:50,864 : Iter 70, training loss: 7.819760
2017-12-25 21:59:53,300 : Iter 80, training loss: 7.776848
2017-12-25 21:59:56,002 : Iter 90, training loss: 8.159592
2017-12-25 21:59:58,481 : Iter 100, training loss: 8.077219
2017-12-25 22:00:01,093 : Iter 110, training loss: 8.032448
2017-12-25 22:00:03,677 : Iter 120, training loss: 8.162786
2017-12-25 22:00:06,295 : Iter 130, training loss: 8.092861
2017-12-25 22:00:08,750 : Iter 140, training loss: 7.999332
2017-12-25 22:00:11,151 : Iter 150, training loss: 7.933695
2017-12-25 22:00:13,532 : Iter 160, training loss: 8.659159
2017-12-25 22:00:16,023 : Iter 170, training loss: 8.650875
2017-12-25 22:00:18,621 : Iter 180, training loss: 8.549027
2017-12-25 22:00:21,082 : Iter 190, training loss: 8.560486
2017-12-25 22:00:23,456 : Iter 200, training loss: 8.511652
2017-12-25 22:00:25,922 : Iter 210, training loss: 8.621325
2017-12-25 22:00:28,335 : Iter 220, training loss: 8.521484
2017-12-25 22:00:30,959 : Iter 230, training loss: 8.458573
2017-12-25 22:00:33,544 : Iter 240, training loss: 8.595903
2017-12-25 22:00:36,165 : Iter 250, training loss: 8.598781
2017-12-25 22:00:38,574 : Iter 260, training loss: 8.808664
2017-12-25 22:00:41,114 : Iter 270, training loss: 8.764023
2017-12-25 22:00:43,622 : Iter 280, training loss: 8.845996
2017-12-25 22:00:46,025 : Iter 290, training loss: 8.867277
2017-12-25 22:00:48,535 : Iter 300, training loss: 8.809604
2017-12-25 22:00:51,123 : Iter 310, training loss: 8.865892
2017-12-25 22:00:53,595 : Iter 320, training loss: 8.841532
2017-12-25 22:00:56,121 : Iter 330, training loss: 8.886597
2017-12-25 22:00:58,384 : Iter 340, training loss: 8.886956
2017-12-25 22:01:00,875 : Iter 350, training loss: 8.900058
2017-12-25 22:01:02,141 : Epoch 7, training loss: 8.865854
2017-12-25 22:01:03,629 : Epoch 7, validation loss: 11.862123
2017-12-25 22:01:03,737 : Training checkpoint has been saved.
2017-12-25 22:01:03,737 : Epoch 8 started:
2017-12-25 22:01:04,025 : Iter 0, training loss: 4.648566
2017-12-25 22:01:06,382 : Iter 10, training loss: 4.461179
2017-12-25 22:01:08,860 : Iter 20, training loss: 4.296829
2017-12-25 22:01:11,197 : Iter 30, training loss: 4.598042
2017-12-25 22:01:13,838 : Iter 40, training loss: 4.959963
2017-12-25 22:01:16,200 : Iter 50, training loss: 4.889371
2017-12-25 22:01:18,805 : Iter 60, training loss: 5.104654
2017-12-25 22:01:21,531 : Iter 70, training loss: 5.333742
2017-12-25 22:01:24,034 : Iter 80, training loss: 5.265292
2017-12-25 22:01:26,503 : Iter 90, training loss: 5.207882
2017-12-25 22:01:28,959 : Iter 100, training loss: 5.364941
2017-12-25 22:01:31,639 : Iter 110, training loss: 6.241066
2017-12-25 22:01:33,956 : Iter 120, training loss: 6.179772
2017-12-25 22:01:36,457 : Iter 130, training loss: 6.133707
2017-12-25 22:01:39,083 : Iter 140, training loss: 6.481598
2017-12-25 22:01:41,657 : Iter 150, training loss: 6.391307
2017-12-25 22:01:44,052 : Iter 160, training loss: 6.851504
2017-12-25 22:01:46,408 : Iter 170, training loss: 6.758947
2017-12-25 22:01:49,073 : Iter 180, training loss: 6.793047
2017-12-25 22:01:51,501 : Iter 190, training loss: 6.748804
2017-12-25 22:01:53,913 : Iter 200, training loss: 6.810757
2017-12-25 22:01:56,508 : Iter 210, training loss: 6.708131
2017-12-25 22:01:59,122 : Iter 220, training loss: 6.631666
2017-12-25 22:02:01,624 : Iter 230, training loss: 6.600673
2017-12-25 22:02:04,042 : Iter 240, training loss: 6.570847
2017-12-25 22:02:06,450 : Iter 250, training loss: 6.685591
2017-12-25 22:02:08,925 : Iter 260, training loss: 6.633593
2017-12-25 22:02:11,435 : Iter 270, training loss: 6.674446
2017-12-25 22:02:13,847 : Iter 280, training loss: 6.658927
2017-12-25 22:02:16,270 : Iter 290, training loss: 6.697230
2017-12-25 22:02:18,716 : Iter 300, training loss: 6.687320
2017-12-25 22:02:21,227 : Iter 310, training loss: 6.713543
2017-12-25 22:02:23,781 : Iter 320, training loss: 6.817879
2017-12-25 22:02:26,107 : Iter 330, training loss: 6.786943
2017-12-25 22:02:28,670 : Iter 340, training loss: 6.854511
2017-12-25 22:02:31,045 : Iter 350, training loss: 6.838334
2017-12-25 22:02:32,304 : Epoch 8, training loss: 6.836592
2017-12-25 22:02:33,841 : Epoch 8, validation loss: 12.784795
2017-12-25 22:02:33,939 : Training checkpoint has been saved.
2017-12-25 22:02:33,940 : Epoch 9 started:
2017-12-25 22:02:34,193 : Iter 0, training loss: 4.510450
2017-12-25 22:02:36,890 : Iter 10, training loss: 5.743207
2017-12-25 22:02:39,340 : Iter 20, training loss: 5.031512
2017-12-25 22:02:41,799 : Iter 30, training loss: 5.027545
2017-12-25 22:02:44,310 : Iter 40, training loss: 5.307077
2017-12-25 22:02:46,919 : Iter 50, training loss: 5.016960
2017-12-25 22:02:49,510 : Iter 60, training loss: 4.894689
2017-12-25 22:02:52,077 : Iter 70, training loss: 4.763106
2017-12-25 22:02:54,447 : Iter 80, training loss: 4.668839
2017-12-25 22:02:56,900 : Iter 90, training loss: 4.771946
2017-12-25 22:02:59,522 : Iter 100, training loss: 4.697773
2017-12-25 22:03:02,045 : Iter 110, training loss: 4.685724
2017-12-25 22:03:04,625 : Iter 120, training loss: 4.625811
2017-12-25 22:03:07,244 : Iter 130, training loss: 4.923941
2017-12-25 22:03:09,900 : Iter 140, training loss: 4.964286
2017-12-25 22:03:12,395 : Iter 150, training loss: 4.903165
2017-12-25 22:03:14,857 : Iter 160, training loss: 4.863714
2017-12-25 22:03:17,285 : Iter 170, training loss: 5.007324
2017-12-25 22:03:19,745 : Iter 180, training loss: 4.946495
2017-12-25 22:03:22,225 : Iter 190, training loss: 4.882362
2017-12-25 22:03:24,725 : Iter 200, training loss: 4.851460
2017-12-25 22:03:27,240 : Iter 210, training loss: 4.793427
2017-12-25 22:03:29,865 : Iter 220, training loss: 4.743603
2017-12-25 22:03:32,197 : Iter 230, training loss: 4.960009
2017-12-25 22:03:34,841 : Iter 240, training loss: 4.930627
2017-12-25 22:03:37,244 : Iter 250, training loss: 4.894113
2017-12-25 22:03:39,556 : Iter 260, training loss: 4.857127
2017-12-25 22:03:41,869 : Iter 270, training loss: 4.894441
2017-12-25 22:03:44,223 : Iter 280, training loss: 4.931646
2017-12-25 22:03:46,727 : Iter 290, training loss: 4.923901
2017-12-25 22:03:49,162 : Iter 300, training loss: 4.914954
2017-12-25 22:03:51,725 : Iter 310, training loss: 4.896072
2017-12-25 22:03:54,160 : Iter 320, training loss: 4.866990
2017-12-25 22:03:56,576 : Iter 330, training loss: 4.883030
2017-12-25 22:03:59,392 : Iter 340, training loss: 4.876581
2017-12-25 22:04:01,796 : Iter 350, training loss: 4.849948
2017-12-25 22:04:02,934 : Epoch 9, training loss: 4.853296
2017-12-25 22:04:04,390 : Epoch 9, validation loss: 11.243053
2017-12-25 22:04:04,647 : Training checkpoint has been saved.
2017-12-25 22:04:04,661 : Training process has been finished.
